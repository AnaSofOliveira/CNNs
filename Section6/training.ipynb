{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course:  Convolutional Neural Networks for Image Classification\n",
    "\n",
    "## Section-6\n",
    "### Train designed CNNs models in Keras\n",
    "\n",
    "**Description:**  \n",
    "*Run training process for all developed models and all prepared datasets  \n",
    "Save trained models and trained weights*\n",
    "\n",
    "**File:** *training.ipynb*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm:\n",
    "\n",
    "**--> Step 1:** Load saved CNN model  \n",
    "**--> Step 2:** Set up learning rate & epochs  \n",
    "**--> Step 3: Train loaded model on all preprocessed datasets**  \n",
    "**--> Step 4:** Show and plot results  \n",
    "\n",
    "\n",
    "**Result:**  \n",
    "- Binary files with saved weights  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing needed libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint, Callback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up full paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full or absolute path to 'Section4' with preprocessed datasets\n",
    "# (!) On Windows, the path should look like following:\n",
    "# r'C:\\Users\\your_name\\PycharmProjects\\CNNCourse\\Section4'\n",
    "# or:\n",
    "# 'C:\\\\Users\\\\your_name\\\\PycharmProjects\\\\CNNCourse\\\\Section4'\n",
    "full_path_to_Section4 = \\\n",
    "    'C:\\\\Users\\\\anaso\\\\OneDrive - Instituto Superior de Engenharia de Lisboa\\\\CNNCourse\\\\Section4'\n",
    "\n",
    "\n",
    "# Full or absolute path to 'Section5' with designed models\n",
    "# (!) On Windows, the path should look like following:\n",
    "# r'C:\\Users\\your_name\\PycharmProjects\\CNNCourse\\Section5'\n",
    "# or:\n",
    "# 'C:\\\\Users\\\\your_name\\\\PycharmProjects\\\\CNNCourse\\\\Section5'\n",
    "full_path_to_Section5 = \\\n",
    "    'C:\\\\Users\\\\anaso\\\\OneDrive - Instituto Superior de Engenharia de Lisboa\\\\CNNCourse\\\\Section5'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom dataset, 1st model\n",
    "\n",
    "## Step 1: Loading saved 1st model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models are successfully loaded\n"
     ]
    }
   ],
   "source": [
    "# Defining lists to collect models in\n",
    "model_rgb = []\n",
    "model_gray = []\n",
    "\n",
    "\n",
    "# Loading 1st model for custom dataset\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "for i in range(2):\n",
    "    model_rgb.append(load_model(full_path_to_Section5 + '/' + \n",
    "                                'custom' + '/' + \n",
    "                                'model_1_custom_rgb.h5'))\n",
    "    \n",
    "    model_gray.append(load_model(full_path_to_Section5 + '/' + \n",
    "                                 'custom' + '/' + \n",
    "                                 'model_1_custom_gray.h5'))\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Models are successfully loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 64, 64, 3)\n",
      "\n",
      "(None, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "# Showing models' input shapes\n",
    "print(model_rgb[0].layers[0].input_shape)\n",
    "print()\n",
    "print(model_gray[0].layers[0].input_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom dataset, 1st model\n",
    "\n",
    "## Step 2: Setting up learning rate & epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of epochs and schedule for learning rate are set successfully\n"
     ]
    }
   ],
   "source": [
    "# Defining number of epochs\n",
    "epochs = 50\n",
    "\n",
    "\n",
    "# Defining schedule to update learning rate\n",
    "learning_rate = LearningRateScheduler(lambda x: 1e-2 * 0.95 ** (x + epochs), verbose=1)\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Number of epochs and schedule for learning rate are set successfully')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom dataset, 1st model\n",
    "\n",
    "## Step 3: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Following dataset is successfully opened:         dataset_custom_rgb_255_mean.hdf5\n",
      "Binary matrices are successfully created:         dataset_custom_rgb_255_mean.hdf5\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Schedule to save best weights is created:         dataset_custom_rgb_255_mean.hdf5\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0007694497527671316.\n",
      "Epoch 1/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.3439 - accuracy: 0.3863\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.45495, saving model to custom\\w_1_custom_rgb_255_mean.h5\n",
      "66/66 [==============================] - 85s 1s/step - loss: 1.3439 - accuracy: 0.3863 - val_loss: 1.2293 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0007309772651287749.\n",
      "Epoch 2/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1648 - accuracy: 0.4908\n",
      "Epoch 00002: val_accuracy improved from 0.45495 to 0.48796, saving model to custom\\w_1_custom_rgb_255_mean.h5\n",
      "66/66 [==============================] - 96s 1s/step - loss: 1.1648 - accuracy: 0.4908 - val_loss: 1.1454 - val_accuracy: 0.4880\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.000694428401872336.\n",
      "Epoch 3/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0850 - accuracy: 0.5428\n",
      "Epoch 00003: val_accuracy improved from 0.48796 to 0.52632, saving model to custom\\w_1_custom_rgb_255_mean.h5\n",
      "66/66 [==============================] - 84s 1s/step - loss: 1.0850 - accuracy: 0.5428 - val_loss: 1.0491 - val_accuracy: 0.5263\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0006597069817787193.\n",
      "Epoch 4/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9906 - accuracy: 0.5675\n",
      "Epoch 00004: val_accuracy improved from 0.52632 to 0.59322, saving model to custom\\w_1_custom_rgb_255_mean.h5\n",
      "66/66 [==============================] - 85s 1s/step - loss: 0.9906 - accuracy: 0.5675 - val_loss: 0.9164 - val_accuracy: 0.5932\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0006267216326897833.\n",
      "Epoch 5/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9218 - accuracy: 0.6094\n",
      "Epoch 00005: val_accuracy improved from 0.59322 to 0.60036, saving model to custom\\w_1_custom_rgb_255_mean.h5\n",
      "66/66 [==============================] - 84s 1s/step - loss: 0.9218 - accuracy: 0.6094 - val_loss: 0.9136 - val_accuracy: 0.6004\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0005953855510552941.\n",
      "Epoch 6/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8486 - accuracy: 0.6372\n",
      "Epoch 00006: val_accuracy improved from 0.60036 to 0.63604, saving model to custom\\w_1_custom_rgb_255_mean.h5\n",
      "66/66 [==============================] - 85s 1s/step - loss: 0.8486 - accuracy: 0.6372 - val_loss: 0.9143 - val_accuracy: 0.6360\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0005656162735025293.\n",
      "Epoch 7/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7845 - accuracy: 0.6699\n",
      "Epoch 00007: val_accuracy did not improve from 0.63604\n",
      "66/66 [==============================] - 84s 1s/step - loss: 0.7845 - accuracy: 0.6699 - val_loss: 0.8925 - val_accuracy: 0.6343\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0005373354598274028.\n",
      "Epoch 8/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7644 - accuracy: 0.6858\n",
      "Epoch 00008: val_accuracy improved from 0.63604 to 0.66012, saving model to custom\\w_1_custom_rgb_255_mean.h5\n",
      "66/66 [==============================] - 82s 1s/step - loss: 0.7644 - accuracy: 0.6858 - val_loss: 0.8188 - val_accuracy: 0.6601\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0005104686868360326.\n",
      "Epoch 9/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6705 - accuracy: 0.7246\n",
      "Epoch 00009: val_accuracy improved from 0.66012 to 0.67172, saving model to custom\\w_1_custom_rgb_255_mean.h5\n",
      "66/66 [==============================] - 81s 1s/step - loss: 0.6705 - accuracy: 0.7246 - val_loss: 0.7926 - val_accuracy: 0.6717\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.00048494525249423106.\n",
      "Epoch 10/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6278 - accuracy: 0.7414\n",
      "Epoch 00010: val_accuracy did not improve from 0.67172\n",
      "66/66 [==============================] - 79s 1s/step - loss: 0.6278 - accuracy: 0.7414 - val_loss: 0.8336 - val_accuracy: 0.6699\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.00046069798986951945.\n",
      "Epoch 11/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5593 - accuracy: 0.7738\n",
      "Epoch 00011: val_accuracy improved from 0.67172 to 0.67529, saving model to custom\\w_1_custom_rgb_255_mean.h5\n",
      "66/66 [==============================] - 77s 1s/step - loss: 0.5593 - accuracy: 0.7738 - val_loss: 0.8751 - val_accuracy: 0.6753\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.0004376630903760435.\n",
      "Epoch 12/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5101 - accuracy: 0.7946\n",
      "Epoch 00012: val_accuracy improved from 0.67529 to 0.69670, saving model to custom\\w_1_custom_rgb_255_mean.h5\n",
      "66/66 [==============================] - 79s 1s/step - loss: 0.5101 - accuracy: 0.7946 - val_loss: 0.8385 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.00041577993585724133.\n",
      "Epoch 13/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4479 - accuracy: 0.8212\n",
      "Epoch 00013: val_accuracy did not improve from 0.69670\n",
      "66/66 [==============================] - 79s 1s/step - loss: 0.4479 - accuracy: 0.8212 - val_loss: 0.8519 - val_accuracy: 0.6851\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.00039499093906437917.\n",
      "Epoch 14/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3783 - accuracy: 0.8539\n",
      "Epoch 00014: val_accuracy did not improve from 0.69670\n",
      "66/66 [==============================] - 73s 1s/step - loss: 0.3783 - accuracy: 0.8539 - val_loss: 1.0139 - val_accuracy: 0.6744\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.00037524139211116017.\n",
      "Epoch 15/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3246 - accuracy: 0.8750\n",
      "Epoch 00015: val_accuracy did not improve from 0.69670\n",
      "66/66 [==============================] - 576s 9s/step - loss: 0.3246 - accuracy: 0.8750 - val_loss: 0.9424 - val_accuracy: 0.6744\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.0003564793225056022.\n",
      "Epoch 16/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2446 - accuracy: 0.9080\n",
      "Epoch 00016: val_accuracy did not improve from 0.69670\n",
      "66/66 [==============================] - 81s 1s/step - loss: 0.2446 - accuracy: 0.9080 - val_loss: 1.0764 - val_accuracy: 0.6887\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.00033865535638032205.\n",
      "Epoch 17/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2051 - accuracy: 0.9309\n",
      "Epoch 00017: val_accuracy did not improve from 0.69670\n",
      "66/66 [==============================] - 85s 1s/step - loss: 0.2051 - accuracy: 0.9309 - val_loss: 1.1385 - val_accuracy: 0.6744\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.00032172258856130595.\n",
      "Epoch 18/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.1499 - accuracy: 0.9477\n",
      "Epoch 00018: val_accuracy did not improve from 0.69670\n",
      "66/66 [==============================] - 88s 1s/step - loss: 0.1499 - accuracy: 0.9477 - val_loss: 1.2422 - val_accuracy: 0.6780\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.0003056364591332406.\n",
      "Epoch 19/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.1409 - accuracy: 0.9542\n",
      "Epoch 00019: val_accuracy did not improve from 0.69670\n",
      "66/66 [==============================] - 85s 1s/step - loss: 0.1409 - accuracy: 0.9542 - val_loss: 1.2434 - val_accuracy: 0.6896\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.0002903546361765786.\n",
      "Epoch 20/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0962 - accuracy: 0.9679\n",
      "Epoch 00020: val_accuracy did not improve from 0.69670\n",
      "66/66 [==============================] - 87s 1s/step - loss: 0.0962 - accuracy: 0.9679 - val_loss: 1.3627 - val_accuracy: 0.6780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.00027583690436774964.\n",
      "Epoch 21/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0929 - accuracy: 0.9694\n",
      "Epoch 00021: val_accuracy did not improve from 0.69670\n",
      "66/66 [==============================] - 95s 1s/step - loss: 0.0929 - accuracy: 0.9694 - val_loss: 1.3886 - val_accuracy: 0.6842\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.0002620450591493622.\n",
      "Epoch 22/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0675 - accuracy: 0.9789\n",
      "Epoch 00022: val_accuracy improved from 0.69670 to 0.69938, saving model to custom\\w_1_custom_rgb_255_mean.h5\n",
      "66/66 [==============================] - 102s 2s/step - loss: 0.0675 - accuracy: 0.9789 - val_loss: 1.4337 - val_accuracy: 0.6994\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.00024894280619189405.\n",
      "Epoch 23/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0482 - accuracy: 0.9878\n",
      "Epoch 00023: val_accuracy did not improve from 0.69938\n",
      "66/66 [==============================] - 102s 2s/step - loss: 0.0482 - accuracy: 0.9878 - val_loss: 1.6252 - val_accuracy: 0.6887\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.00023649566588229934.\n",
      "Epoch 24/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0430 - accuracy: 0.9887\n",
      "Epoch 00024: val_accuracy did not improve from 0.69938\n",
      "66/66 [==============================] - 102s 2s/step - loss: 0.0430 - accuracy: 0.9887 - val_loss: 1.6137 - val_accuracy: 0.6985\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.00022467088258818434.\n",
      "Epoch 25/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0394 - accuracy: 0.9878\n",
      "Epoch 00025: val_accuracy did not improve from 0.69938\n",
      "66/66 [==============================] - 103s 2s/step - loss: 0.0394 - accuracy: 0.9878 - val_loss: 1.6027 - val_accuracy: 0.6905\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.0002134373384587751.\n",
      "Epoch 26/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0460 - accuracy: 0.9835\n",
      "Epoch 00026: val_accuracy improved from 0.69938 to 0.70562, saving model to custom\\w_1_custom_rgb_255_mean.h5\n",
      "66/66 [==============================] - 102s 2s/step - loss: 0.0460 - accuracy: 0.9835 - val_loss: 1.5670 - val_accuracy: 0.7056\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.00020276547153583635.\n",
      "Epoch 27/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0289 - accuracy: 0.9933\n",
      "Epoch 00027: val_accuracy did not improve from 0.70562\n",
      "66/66 [==============================] - 107s 2s/step - loss: 0.0289 - accuracy: 0.9933 - val_loss: 1.7053 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.0001926271979590445.\n",
      "Epoch 28/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0308 - accuracy: 0.9917\n",
      "Epoch 00028: val_accuracy did not improve from 0.70562\n",
      "66/66 [==============================] - 104s 2s/step - loss: 0.0308 - accuracy: 0.9917 - val_loss: 1.7304 - val_accuracy: 0.6905\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.0001829958380610923.\n",
      "Epoch 29/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0283 - accuracy: 0.9905\n",
      "Epoch 00029: val_accuracy did not improve from 0.70562\n",
      "66/66 [==============================] - 106s 2s/step - loss: 0.0283 - accuracy: 0.9905 - val_loss: 1.8021 - val_accuracy: 0.6789\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.00017384604615803767.\n",
      "Epoch 30/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0231 - accuracy: 0.9939\n",
      "Epoch 00030: val_accuracy did not improve from 0.70562\n",
      "66/66 [==============================] - 101s 2s/step - loss: 0.0231 - accuracy: 0.9939 - val_loss: 1.7877 - val_accuracy: 0.6922\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.0001651537438501358.\n",
      "Epoch 31/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 0.9963\n",
      "Epoch 00031: val_accuracy did not improve from 0.70562\n",
      "66/66 [==============================] - 103s 2s/step - loss: 0.0148 - accuracy: 0.9963 - val_loss: 1.8745 - val_accuracy: 0.6940\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.00015689605665762898.\n",
      "Epoch 32/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0160 - accuracy: 0.9966\n",
      "Epoch 00032: val_accuracy did not improve from 0.70562\n",
      "66/66 [==============================] - 106s 2s/step - loss: 0.0160 - accuracy: 0.9966 - val_loss: 1.8542 - val_accuracy: 0.6842\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.00014905125382474752.\n",
      "Epoch 33/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0196 - accuracy: 0.9939\n",
      "Epoch 00033: val_accuracy did not improve from 0.70562\n",
      "66/66 [==============================] - 104s 2s/step - loss: 0.0196 - accuracy: 0.9939 - val_loss: 1.8203 - val_accuracy: 0.6905\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.00014159869113351015.\n",
      "Epoch 34/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0151 - accuracy: 0.9960\n",
      "Epoch 00034: val_accuracy did not improve from 0.70562\n",
      "66/66 [==============================] - 104s 2s/step - loss: 0.0151 - accuracy: 0.9960 - val_loss: 1.9056 - val_accuracy: 0.6994\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.00013451875657683463.\n",
      "Epoch 35/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.9979\n",
      "Epoch 00035: val_accuracy did not improve from 0.70562\n",
      "66/66 [==============================] - 106s 2s/step - loss: 0.0141 - accuracy: 0.9979 - val_loss: 1.9218 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.00012779281874799288.\n",
      "Epoch 36/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0090 - accuracy: 0.9982\n",
      "Epoch 00036: val_accuracy did not improve from 0.70562\n",
      "66/66 [==============================] - 106s 2s/step - loss: 0.0090 - accuracy: 0.9982 - val_loss: 1.9736 - val_accuracy: 0.6931\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 0.00012140317781059325.\n",
      "Epoch 37/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9963\n",
      "Epoch 00037: val_accuracy did not improve from 0.70562\n",
      "66/66 [==============================] - 103s 2s/step - loss: 0.0159 - accuracy: 0.9963 - val_loss: 1.9134 - val_accuracy: 0.6896\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 0.00011533301892006357.\n",
      "Epoch 38/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0172 - accuracy: 0.9960\n",
      "Epoch 00038: val_accuracy did not improve from 0.70562\n",
      "66/66 [==============================] - 104s 2s/step - loss: 0.0172 - accuracy: 0.9960 - val_loss: 1.8815 - val_accuracy: 0.6896\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 0.00010956636797406039.\n",
      "Epoch 39/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0102 - accuracy: 0.9979\n",
      "Epoch 00039: val_accuracy did not improve from 0.70562\n",
      "66/66 [==============================] - 104s 2s/step - loss: 0.0102 - accuracy: 0.9979 - val_loss: 1.8882 - val_accuracy: 0.6851\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 0.00010408804957535738.\n",
      "Epoch 40/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0096 - accuracy: 0.9976\n",
      "Epoch 00040: val_accuracy did not improve from 0.70562\n",
      "66/66 [==============================] - 106s 2s/step - loss: 0.0096 - accuracy: 0.9976 - val_loss: 1.9667 - val_accuracy: 0.6798\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 9.88836470965895e-05.\n",
      "Epoch 41/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0066 - accuracy: 0.9991\n",
      "Epoch 00041: val_accuracy did not improve from 0.70562\n",
      "66/66 [==============================] - 105s 2s/step - loss: 0.0066 - accuracy: 0.9991 - val_loss: 1.9734 - val_accuracy: 0.6896\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 9.393946474176e-05.\n",
      "Epoch 42/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0082 - accuracy: 0.9985\n",
      "Epoch 00042: val_accuracy did not improve from 0.70562\n",
      "66/66 [==============================] - 99s 2s/step - loss: 0.0082 - accuracy: 0.9985 - val_loss: 1.9535 - val_accuracy: 0.6815\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 8.924249150467202e-05.\n",
      "Epoch 43/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - ETA: 0s - loss: 0.0083 - accuracy: 0.9979\n",
      "Epoch 00043: val_accuracy did not improve from 0.70562\n",
      "66/66 [==============================] - 91s 1s/step - loss: 0.0083 - accuracy: 0.9979 - val_loss: 2.0526 - val_accuracy: 0.6860\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 8.478036692943841e-05.\n",
      "Epoch 44/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0067 - accuracy: 0.9988\n",
      "Epoch 00044: val_accuracy did not improve from 0.70562\n",
      "66/66 [==============================] - 86s 1s/step - loss: 0.0067 - accuracy: 0.9988 - val_loss: 2.0284 - val_accuracy: 0.6896\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 8.054134858296649e-05.\n",
      "Epoch 45/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0050 - accuracy: 0.9997\n",
      "Epoch 00045: val_accuracy did not improve from 0.70562\n",
      "66/66 [==============================] - 89s 1s/step - loss: 0.0050 - accuracy: 0.9997 - val_loss: 2.0660 - val_accuracy: 0.6931\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 7.651428115381816e-05.\n",
      "Epoch 46/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0056 - accuracy: 0.9991\n",
      "Epoch 00046: val_accuracy did not improve from 0.70562\n",
      "66/66 [==============================] - 86s 1s/step - loss: 0.0056 - accuracy: 0.9991 - val_loss: 2.0594 - val_accuracy: 0.6887\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 7.268856709612724e-05.\n",
      "Epoch 47/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0076 - accuracy: 0.9969\n",
      "Epoch 00047: val_accuracy did not improve from 0.70562\n",
      "66/66 [==============================] - 84s 1s/step - loss: 0.0076 - accuracy: 0.9969 - val_loss: 2.0921 - val_accuracy: 0.6887\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 6.905413874132087e-05.\n",
      "Epoch 48/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0077 - accuracy: 0.9976\n",
      "Epoch 00048: val_accuracy did not improve from 0.70562\n",
      "66/66 [==============================] - 94s 1s/step - loss: 0.0077 - accuracy: 0.9976 - val_loss: 2.0730 - val_accuracy: 0.6887\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 6.560143180425483e-05.\n",
      "Epoch 49/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0039 - accuracy: 0.9997\n",
      "Epoch 00049: val_accuracy did not improve from 0.70562\n",
      "66/66 [==============================] - 98s 1s/step - loss: 0.0039 - accuracy: 0.9997 - val_loss: 2.0935 - val_accuracy: 0.6958\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 6.232136021404208e-05.\n",
      "Epoch 50/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0040 - accuracy: 0.9994\n",
      "Epoch 00050: val_accuracy did not improve from 0.70562\n",
      "66/66 [==============================] - 99s 2s/step - loss: 0.0040 - accuracy: 0.9994 - val_loss: 2.1035 - val_accuracy: 0.6967\n",
      "1st model for RGB is successfully trained on:     dataset_custom_rgb_255_mean.hdf5\n",
      "Trained weights for RGB are saved successfully:   w_1_custom_rgb_255_mean.h5\n",
      "\n",
      "Following dataset is successfully opened:         dataset_custom_rgb_255_mean_std.hdf5\n",
      "Binary matrices are successfully created:         dataset_custom_rgb_255_mean_std.hdf5\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Schedule to save best weights is created:         dataset_custom_rgb_255_mean_std.hdf5\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0007694497527671316.\n",
      "Epoch 1/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.4130 - accuracy: 0.3787\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.39072, saving model to custom\\w_1_custom_rgb_255_mean_std.h5\n",
      "66/66 [==============================] - 103s 2s/step - loss: 1.4130 - accuracy: 0.3787 - val_loss: 1.3090 - val_accuracy: 0.3907\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0007309772651287749.\n",
      "Epoch 2/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.2165 - accuracy: 0.4722\n",
      "Epoch 00002: val_accuracy improved from 0.39072 to 0.46922, saving model to custom\\w_1_custom_rgb_255_mean_std.h5\n",
      "66/66 [==============================] - 96s 1s/step - loss: 1.2165 - accuracy: 0.4722 - val_loss: 1.2165 - val_accuracy: 0.4692\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.000694428401872336.\n",
      "Epoch 3/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1343 - accuracy: 0.5104\n",
      "Epoch 00003: val_accuracy improved from 0.46922 to 0.53524, saving model to custom\\w_1_custom_rgb_255_mean_std.h5\n",
      "66/66 [==============================] - 101s 2s/step - loss: 1.1343 - accuracy: 0.5104 - val_loss: 1.0559 - val_accuracy: 0.5352\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0006597069817787193.\n",
      "Epoch 4/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0526 - accuracy: 0.5443\n",
      "Epoch 00004: val_accuracy improved from 0.53524 to 0.58876, saving model to custom\\w_1_custom_rgb_255_mean_std.h5\n",
      "66/66 [==============================] - 98s 1s/step - loss: 1.0526 - accuracy: 0.5443 - val_loss: 0.9724 - val_accuracy: 0.5888\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0006267216326897833.\n",
      "Epoch 5/50\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9837 - accuracy: 0.5801\n",
      "Epoch 00005: val_accuracy improved from 0.58876 to 0.59857, saving model to custom\\w_1_custom_rgb_255_mean_std.h5\n",
      "66/66 [==============================] - 99s 1s/step - loss: 0.9837 - accuracy: 0.5801 - val_loss: 0.9518 - val_accuracy: 0.5986\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0005953855510552941.\n",
      "Epoch 6/50\n",
      "46/66 [===================>..........] - ETA: 28s - loss: 0.9177 - accuracy: 0.6078"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13624\\2925969087.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     91\u001b[0m                                 \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_validation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_validation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m                                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_weights\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m                                 verbose=1)\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\cnngpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\cnngpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\cnngpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\cnngpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\cnngpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\cnngpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\cnngpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\cnngpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\.conda\\envs\\cnngpu\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# If you're using Nvidia GPU and 'cnngpu' environment, there might be an issue like:\n",
    "'''Failed to get convolution algorithm. This is probably because cuDNN failed to initialize'''\n",
    "# In this case, close all Jupyter Notebooks, close Terminal Window or Anaconda Prompt\n",
    "# Open again just this one Jupyter Notebook and run it\n",
    "\n",
    "\n",
    "# Preparing list with datasets' names\n",
    "datasets = ['dataset_custom_rgb_255_mean.hdf5',\n",
    "            'dataset_custom_rgb_255_mean_std.hdf5',\n",
    "            'dataset_custom_gray_255_mean.hdf5',\n",
    "            'dataset_custom_gray_255_mean_std.hdf5']\n",
    "\n",
    "\n",
    "# Defining list to collect results in\n",
    "h = []\n",
    "\n",
    "\n",
    "# Training 1st model with all custom datasets in a loop\n",
    "for i in range(4):    \n",
    "    # Opening saved custom dataset from HDF5 binary file\n",
    "    # Initiating File object\n",
    "    # Opening file in reading mode by 'r'\n",
    "    # (!) On Windows, it might need to change\n",
    "    # this: + '/' +\n",
    "    # to this: + '\\' +\n",
    "    # or to this: + '\\\\' +\n",
    "    with h5py.File(full_path_to_Section4 + '/' + 'custom' + '/' + datasets[i], 'r') as f:\n",
    "        # Extracting saved arrays for training by appropriate keys\n",
    "        # Saving them into new variables\n",
    "        x_train = f['x_train']  # HDF5 dataset\n",
    "        y_train = f['y_train']  # HDF5 dataset\n",
    "        # Converting them into Numpy arrays\n",
    "        x_train = np.array(x_train)  # Numpy arrays\n",
    "        y_train = np.array(y_train)  # Numpy arrays\n",
    "\n",
    "        # Extracting saved arrays for validation by appropriate keys\n",
    "        # Saving them into new variables\n",
    "        x_validation = f['x_validation']  # HDF5 dataset\n",
    "        y_validation = f['y_validation']  # HDF5 dataset\n",
    "        # Converting them into Numpy arrays\n",
    "        x_validation = np.array(x_validation)  # Numpy arrays\n",
    "        y_validation = np.array(y_validation)  # Numpy arrays\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Following dataset is successfully opened:        ', datasets[i])\n",
    "    \n",
    "    \n",
    "    # Preparing classes to be passed into the model\n",
    "    # Transforming them from vectors to binary matrices\n",
    "    # It is needed to set relationship between classes to be understood by the algorithm\n",
    "    # Such format is commonly used in training and predicting\n",
    "    y_train = to_categorical(y_train, num_classes = 5)\n",
    "    y_validation = to_categorical(y_validation, num_classes = 5)\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Binary matrices are successfully created:        ', datasets[i])\n",
    " \n",
    "\n",
    "    # Preparing filepath to save best weights\n",
    "    # (!) On Windows, it might need to change\n",
    "    # this: + '/' +\n",
    "    # to this: + '\\' +\n",
    "    # or to this: + '\\\\' +\n",
    "    best_weights_filepath = 'custom' + '/' + 'w_1' + datasets[i][7:-5] + '.h5'\n",
    "    \n",
    "    # Formatting options to save all weights for every epoch\n",
    "    # 'custom' + '/' + 'w_1' + datasets[i][7:-5] + '_{epoch:02d}_{val_accuracy:.4f}' + '.h5'\n",
    "    \n",
    "    # Defining schedule to save best weights\n",
    "    best_weights = ModelCheckpoint(filepath=best_weights_filepath,\n",
    "                                   save_weights_only=True,                                   \n",
    "                                   monitor='val_accuracy',\n",
    "                                   mode='max',\n",
    "                                   save_best_only=True,\n",
    "                                   period=1,\n",
    "                                   verbose=1)\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Schedule to save best weights is created:        ', datasets[i])\n",
    "\n",
    "    \n",
    "    # Checking if RGB dataset is opened\n",
    "    if i <= 1:\n",
    "        # Training RGB model with current dataset\n",
    "        temp = model_rgb[i].fit(x_train, y_train,\n",
    "                                batch_size=50,\n",
    "                                epochs=epochs,\n",
    "                                validation_data=(x_validation, y_validation),\n",
    "                                callbacks=[learning_rate, best_weights],\n",
    "                                verbose=1)\n",
    "\n",
    "        \n",
    "        # Adding results of 1st model for current RGB dataset in the list\n",
    "        h.append(temp)\n",
    "        \n",
    "        \n",
    "        # Check points\n",
    "        print('1st model for RGB is successfully trained on:    ', datasets[i])\n",
    "        print('Trained weights for RGB are saved successfully:  ', 'w_1' + datasets[i][7:-5] + '.h5')\n",
    "        print()\n",
    "    \n",
    "    # Checking if GRAY dataset is opened\n",
    "    elif i >= 2:\n",
    "        # Training GRAY model with current dataset\n",
    "        temp = model_gray[i-2].fit(x_train, y_train,\n",
    "                                   batch_size=50,\n",
    "                                   epochs=epochs,\n",
    "                                   validation_data=(x_validation, y_validation),\n",
    "                                   callbacks=[learning_rate, best_weights],\n",
    "                                   verbose=1)\n",
    "\n",
    "        \n",
    "        # Adding results of 1st model for current GRAY dataset in the list\n",
    "        h.append(temp)\n",
    "        \n",
    "        \n",
    "        # Check points\n",
    "        print('1st model for GRAY is successfully trained on:   ', datasets[i])\n",
    "        print('Trained weights for GRAY are saved successfully: ', 'w_1' + datasets[i][7:-5] + '.h5')\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom dataset, 1st model\n",
    "\n",
    "## Step 4: Showing and plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resulted accuracies of all custom datasets for 1st model\n",
    "for i in range(4):\n",
    "    print('T: {0:.5f},  V: {1:.5f},  D: {2}'.format(max(h[i].history['accuracy']),\n",
    "                                                    max(h[i].history['val_accuracy']),\n",
    "                                                    datasets[i][8:-5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing other parameters that history holds\n",
    "print(h[0].params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "\n",
    "\n",
    "# Plotting accuracies of all custom datasets for 1st model\n",
    "plt.plot(h[0].history['val_accuracy'], '-o')\n",
    "plt.plot(h[1].history['val_accuracy'], '-o')\n",
    "plt.plot(h[2].history['val_accuracy'], '-o')\n",
    "plt.plot(h[3].history['val_accuracy'], '-o')\n",
    "\n",
    "\n",
    "# Showing legend\n",
    "plt.legend(['rgb_255_mean', 'rgb_255_mean_std',\n",
    "            'gray_255_mean', 'gray_255_mean_std'],\n",
    "           loc='lower right',\n",
    "           fontsize='xx-large')\n",
    "\n",
    "\n",
    "# Giving name to axes\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "\n",
    "\n",
    "# Setting limit along Y axis\n",
    "plt.ylim(0.61, 0.694)\n",
    "\n",
    "\n",
    "# Giving name to the plot\n",
    "plt.title('Validation accuracies of 1st model for Custom Datasets', fontsize=16)\n",
    "\n",
    "\n",
    "# Saving plot\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "plt.savefig('custom' + '/' + 'validation_model_1_custom_dataset.png', dpi=500)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "\n",
    "\n",
    "# Plotting training and validation losses of all custom datasets for 1st model\n",
    "plt.plot(h[0].history['loss'], '-ob')\n",
    "plt.plot(h[1].history['loss'], '-og')\n",
    "plt.plot(h[2].history['loss'], '-or')\n",
    "plt.plot(h[3].history['loss'], '-oc')\n",
    "\n",
    "plt.plot(h[0].history['val_loss'], '-ob')\n",
    "plt.plot(h[1].history['val_loss'], '-og')\n",
    "plt.plot(h[2].history['val_loss'], '-or')\n",
    "plt.plot(h[3].history['val_loss'], '-oc')\n",
    "\n",
    "\n",
    "# Showing legend\n",
    "plt.legend(['rgb_255_mean', 'rgb_255_mean_std',\n",
    "            'gray_255_mean', 'gray_255_mean_std'],\n",
    "           loc='center right',\n",
    "           fontsize='large')\n",
    "\n",
    "\n",
    "# Giving name to axes\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "\n",
    "\n",
    "# Giving name to the plot\n",
    "plt.title('Losses of 1st model for Custom Datasets', fontsize=16)\n",
    "\n",
    "\n",
    "# Saving plot\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "plt.savefig('custom' + '/' + 'losses_model_1_custom_dataset.png', dpi=500)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing list of scheduled learning rate for every epoch\n",
    "print(h[0].history['lr'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Plotting scheduled learning rate\n",
    "plt.plot(h[0].history['lr'], '-mo')\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom dataset, 2nd model\n",
    "\n",
    "## Step 1: Loading saved 2nd model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining lists to collect models in\n",
    "model_rgb = []\n",
    "model_gray = []\n",
    "\n",
    "\n",
    "# Loading 2nd model for custom dataset\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "for i in range(2):\n",
    "    model_rgb.append(load_model(full_path_to_Section5 + '/' + \n",
    "                                'custom' + '/' + \n",
    "                                'model_2_custom_rgb.h5'))\n",
    "    \n",
    "    model_gray.append(load_model(full_path_to_Section5 + '/' + \n",
    "                                 'custom' + '/' + \n",
    "                                 'model_2_custom_gray.h5'))\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Models are successfully loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing models' input shapes\n",
    "print(model_rgb[0].layers[0].input_shape)\n",
    "print()\n",
    "print(model_gray[0].layers[0].input_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom dataset, 2nd model\n",
    "\n",
    "## Step 2: Setting up learning rate & epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining number of epochs\n",
    "epochs = 50\n",
    "\n",
    "\n",
    "# Defining schedule to update learning rate\n",
    "learning_rate = LearningRateScheduler(lambda x: 1e-2 * 0.95 ** (x + epochs), verbose=1)\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Number of epochs and schedule for learning rate are set successfully')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom dataset, 2nd model\n",
    "\n",
    "## Step 3: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If you're using Nvidia GPU and 'cnngpu' environment, there might be an issue like:\n",
    "'''Failed to get convolution algorithm. This is probably because cuDNN failed to initialize'''\n",
    "# In this case, close all Jupyter Notebooks, close Terminal Window or Anaconda Prompt\n",
    "# Open again just this one Jupyter Notebook and run it\n",
    "\n",
    "\n",
    "# Preparing list with datasets' names\n",
    "datasets = ['dataset_custom_rgb_255_mean.hdf5',\n",
    "            'dataset_custom_rgb_255_mean_std.hdf5',\n",
    "            'dataset_custom_gray_255_mean.hdf5',\n",
    "            'dataset_custom_gray_255_mean_std.hdf5']\n",
    "\n",
    "\n",
    "# Defining list to collect results in\n",
    "h = []\n",
    "\n",
    "\n",
    "# Training 2nd model with all custom datasets in a loop\n",
    "for i in range(4):    \n",
    "    # Opening saved custom dataset from HDF5 binary file\n",
    "    # Initiating File object\n",
    "    # Opening file in reading mode by 'r'\n",
    "    # (!) On Windows, it might need to change\n",
    "    # this: + '/' +\n",
    "    # to this: + '\\' +\n",
    "    # or to this: + '\\\\' +\n",
    "    with h5py.File(full_path_to_Section4 + '/' + 'custom' + '/' + datasets[i], 'r') as f:\n",
    "        # Extracting saved arrays for training by appropriate keys\n",
    "        # Saving them into new variables\n",
    "        x_train = f['x_train']  # HDF5 dataset\n",
    "        y_train = f['y_train']  # HDF5 dataset\n",
    "        # Converting them into Numpy arrays\n",
    "        x_train = np.array(x_train)  # Numpy arrays\n",
    "        y_train = np.array(y_train)  # Numpy arrays\n",
    "\n",
    "        # Extracting saved arrays for validation by appropriate keys\n",
    "        # Saving them into new variables\n",
    "        x_validation = f['x_validation']  # HDF5 dataset\n",
    "        y_validation = f['y_validation']  # HDF5 dataset\n",
    "        # Converting them into Numpy arrays\n",
    "        x_validation = np.array(x_validation)  # Numpy arrays\n",
    "        y_validation = np.array(y_validation)  # Numpy arrays\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Following dataset is successfully opened:        ', datasets[i])\n",
    "    \n",
    "    \n",
    "    # Preparing classes to be passed into the model\n",
    "    # Transforming them from vectors to binary matrices\n",
    "    # It is needed to set relationship between classes to be understood by the algorithm\n",
    "    # Such format is commonly used in training and predicting\n",
    "    y_train = to_categorical(y_train, num_classes = 5)\n",
    "    y_validation = to_categorical(y_validation, num_classes = 5)\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Binary matrices are successfully created:        ', datasets[i])\n",
    " \n",
    "\n",
    "    # Preparing filepath to save best weights\n",
    "    # (!) On Windows, it might need to change\n",
    "    # this: + '/' +\n",
    "    # to this: + '\\' +\n",
    "    # or to this: + '\\\\' +\n",
    "    best_weights_filepath = 'custom' + '/' + 'w_2' + datasets[i][7:-5] + '.h5'\n",
    "    \n",
    "    # Formatting options to save all weights for every epoch\n",
    "    # 'custom' + '/' + 'w_2' + datasets[i][7:-5] + '_{epoch:02d}_{val_accuracy:.4f}' + '.h5'\n",
    "    \n",
    "    # Defining schedule to save best weights\n",
    "    best_weights = ModelCheckpoint(filepath=best_weights_filepath,\n",
    "                                   save_weights_only=True,                                   \n",
    "                                   monitor='val_accuracy',\n",
    "                                   mode='max',\n",
    "                                   save_best_only=True,\n",
    "                                   period=1,\n",
    "                                   verbose=1)\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Schedule to save best weights is created:        ', datasets[i])\n",
    "    \n",
    "    \n",
    "    # Defining schedule to save intermediate weights\n",
    "    class CustomCallback(Callback):\n",
    "        # Constructor of the class\n",
    "        def __init__(self):\n",
    "            # Defining variable to be as a part of filename\n",
    "            self.filename = 0\n",
    "        \n",
    "        # Function that is called at the end of every batch\n",
    "        def on_train_batch_end(self, batch, logs=None):\n",
    "            # Checking if it is every 10th batch\n",
    "            if batch % 10 == 0:\n",
    "                # Preparing filepath to save intermediate weights\n",
    "                # (!) On Windows, it might need to change\n",
    "                # this: + '/' +\n",
    "                # to this: + '\\' +\n",
    "                # or to this: + '\\\\' +\n",
    "                intermediate_weights_filepath = 'custom' + '/' + \\\n",
    "                                                'intermediate' + '/' + \\\n",
    "                                                '{0:04d}'.format(self.filename) + \\\n",
    "                                                '_w_2' + \\\n",
    "                                                datasets[i][7:-5] + '.h5'\n",
    "                \n",
    "                # Getting weights only for the first convolutional layer\n",
    "                weights_layer_0 = self.model.get_weights()[0]\n",
    "                \n",
    "                # Saving obtained weights into new HDF5 binary file\n",
    "                # Initiating File object\n",
    "                # Creating file with current name\n",
    "                # Opening it in writing mode by 'w'\n",
    "                # (!) On Windows, it might need to change\n",
    "                # this: + '/' +\n",
    "                # to this: + '\\' +\n",
    "                # or to this: + '\\\\' +\n",
    "                with h5py.File(intermediate_weights_filepath, 'w') as f:\n",
    "                    # Calling method to create dataset of given shape and type\n",
    "                    # Saving Numpy array with weights from the first layer\n",
    "                    f.create_dataset('weights_layer_0', data=weights_layer_0, dtype='f')\n",
    "                    \n",
    "                # Increasing variable to be as a part of the next filename\n",
    "                self.filename += 1\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Schedule to save intermediate weights is created:', datasets[i])\n",
    "\n",
    "    \n",
    "    # Checking if RGB dataset is opened\n",
    "    if i <= 1:\n",
    "        # Checking if second RGB dataset is opened\n",
    "        if i == 1:\n",
    "            # Training RGB model with current dataset\n",
    "            temp = model_rgb[i].fit(x_train, y_train,\n",
    "                                    batch_size=50,\n",
    "                                    epochs=epochs,\n",
    "                                    validation_data=(x_validation, y_validation),\n",
    "                                    callbacks=[learning_rate, best_weights, CustomCallback()],\n",
    "                                    verbose=1)\n",
    "        \n",
    "        # Checking if first RGB dataset is opened\n",
    "        else:\n",
    "            # Training RGB model with current dataset\n",
    "            temp = model_rgb[i].fit(x_train, y_train,\n",
    "                                    batch_size=50,\n",
    "                                    epochs=epochs,\n",
    "                                    validation_data=(x_validation, y_validation),\n",
    "                                    callbacks=[learning_rate, best_weights],\n",
    "                                    verbose=1)\n",
    "\n",
    "        \n",
    "        # Adding results of 2nd model for current RGB dataset in the list\n",
    "        h.append(temp)\n",
    "        \n",
    "        \n",
    "        # Check points\n",
    "        print('2nd model for RGB is successfully trained on:    ', datasets[i])\n",
    "        print('Trained weights for RGB are saved successfully:  ', 'w_2' + datasets[i][7:-5] + '.h5')\n",
    "        print()\n",
    "    \n",
    "    # Checking if GRAY dataset is opened\n",
    "    elif i >= 2:\n",
    "        # Training GRAY model with current dataset\n",
    "        temp = model_gray[i-2].fit(x_train, y_train,\n",
    "                                   batch_size=50,\n",
    "                                   epochs=epochs,\n",
    "                                   validation_data=(x_validation, y_validation),\n",
    "                                   callbacks=[learning_rate, best_weights],\n",
    "                                   verbose=1)\n",
    "\n",
    "        \n",
    "        # Adding results of 2nd model for current GRAY dataset in the list\n",
    "        h.append(temp)\n",
    "        \n",
    "        \n",
    "        # Check points\n",
    "        print('2nd model for GRAY is successfully trained on:   ', datasets[i])\n",
    "        print('Trained weights for GRAY are saved successfully: ', 'w_2' + datasets[i][7:-5] + '.h5')\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom dataset, 2nd model\n",
    "\n",
    "## Step 4: Showing and plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resulted accuracies of all custom datasets for 2nd model\n",
    "for i in range(4):\n",
    "    print('T: {0:.5f},  V: {1:.5f},  D: {2}'.format(max(h[i].history['accuracy']),\n",
    "                                                    max(h[i].history['val_accuracy']),\n",
    "                                                    datasets[i][8:-5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing other parameters that history holds\n",
    "print(h[0].params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "\n",
    "\n",
    "# Plotting accuracies of all custom datasets for 2nd model\n",
    "plt.plot(h[0].history['val_accuracy'], '-o')\n",
    "plt.plot(h[1].history['val_accuracy'], '-o')\n",
    "plt.plot(h[2].history['val_accuracy'], '-o')\n",
    "plt.plot(h[3].history['val_accuracy'], '-o')\n",
    "\n",
    "\n",
    "# Showing legend\n",
    "plt.legend(['rgb_255_mean', 'rgb_255_mean_std',\n",
    "            'gray_255_mean', 'gray_255_mean_std'],\n",
    "           loc='lower right',\n",
    "           fontsize='xx-large')\n",
    "\n",
    "\n",
    "# Giving name to axes\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "\n",
    "\n",
    "# Setting limit along Y axis\n",
    "plt.ylim(0.4, 0.615)\n",
    "\n",
    "\n",
    "# Giving name to the plot\n",
    "plt.title('Validation accuracies of 2nd model for Custom Datasets', fontsize=16)\n",
    "\n",
    "\n",
    "# Saving plot\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "plt.savefig('custom' + '/' + 'validation_model_2_custom_dataset.png', dpi=500)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "\n",
    "\n",
    "# Plotting training and validation losses of all custom datasets for 2nd model\n",
    "plt.plot(h[0].history['loss'], '-ob')\n",
    "plt.plot(h[1].history['loss'], '-og')\n",
    "plt.plot(h[2].history['loss'], '-or')\n",
    "plt.plot(h[3].history['loss'], '-oc')\n",
    "\n",
    "plt.plot(h[0].history['val_loss'], '-ob')\n",
    "plt.plot(h[1].history['val_loss'], '-og')\n",
    "plt.plot(h[2].history['val_loss'], '-or')\n",
    "plt.plot(h[3].history['val_loss'], '-oc')\n",
    "\n",
    "\n",
    "# Showing legend\n",
    "plt.legend(['rgb_255_mean', 'rgb_255_mean_std',\n",
    "            'gray_255_mean', 'gray_255_mean_std'],\n",
    "           loc='upper right',\n",
    "           fontsize='large')\n",
    "\n",
    "\n",
    "# Giving name to axes\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "\n",
    "\n",
    "# Giving name to the plot\n",
    "plt.title('Losses of 2nd model for Custom Datasets', fontsize=16)\n",
    "\n",
    "\n",
    "# Saving plot\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "plt.savefig('custom' + '/' + 'losses_model_2_custom_dataset.png', dpi=500)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing list of scheduled learning rate for every epoch\n",
    "print(h[0].history['lr'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Plotting scheduled learning rate\n",
    "plt.plot(h[0].history['lr'], '-mo')\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10 dataset, 1st model\n",
    "\n",
    "## Step 1: Loading saved 1st model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining lists to collect models in\n",
    "model_rgb = []\n",
    "model_gray = []\n",
    "\n",
    "\n",
    "# Loading 1st model for CIFAR-10 dataset\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "for i in range(2):\n",
    "    model_rgb.append(load_model(full_path_to_Section5 + '/' + \n",
    "                                'cifar10' + '/' + \n",
    "                                'model_1_cifar10_rgb.h5'))\n",
    "    \n",
    "    model_gray.append(load_model(full_path_to_Section5 + '/' + \n",
    "                                 'cifar10' + '/' + \n",
    "                                 'model_1_cifar10_gray.h5'))\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Models are successfully loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing models' input shapes\n",
    "print(model_rgb[0].layers[0].input_shape)\n",
    "print()\n",
    "print(model_gray[0].layers[0].input_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10 dataset, 1st model\n",
    "\n",
    "## Step 2: Setting up learning rate & epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining number of epochs\n",
    "epochs = 50\n",
    "\n",
    "\n",
    "# Defining schedule to update learning rate\n",
    "learning_rate = LearningRateScheduler(lambda x: 1e-2 * 0.95 ** (x + epochs), verbose=1)\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Number of epochs and schedule for learning rate are set successfully')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10 dataset, 1st model\n",
    "\n",
    "## Step 3: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If you're using Nvidia GPU and 'cnngpu' environment, there might be an issue like:\n",
    "'''Failed to get convolution algorithm. This is probably because cuDNN failed to initialize'''\n",
    "# In this case, close all Jupyter Notebooks, close Terminal Window or Anaconda Prompt\n",
    "# Open again just this one Jupyter Notebook and run it\n",
    "\n",
    "\n",
    "# Preparing list with datasets' names\n",
    "datasets = ['dataset_cifar10_rgb_255_mean.hdf5',\n",
    "            'dataset_cifar10_rgb_255_mean_std.hdf5',\n",
    "            'dataset_cifar10_gray_255_mean.hdf5',\n",
    "            'dataset_cifar10_gray_255_mean_std.hdf5']\n",
    "\n",
    "\n",
    "# Defining list to collect results in\n",
    "h = []\n",
    "\n",
    "\n",
    "# Training 1st model with all CIFAR-10 datasets in a loop\n",
    "for i in range(4):    \n",
    "    # Opening saved CIFAR-10 dataset from HDF5 binary file\n",
    "    # Initiating File object\n",
    "    # Opening file in reading mode by 'r'\n",
    "    # (!) On Windows, it might need to change\n",
    "    # this: + '/' +\n",
    "    # to this: + '\\' +\n",
    "    # or to this: + '\\\\' +\n",
    "    with h5py.File(full_path_to_Section4 + '/' + 'cifar10' + '/' + datasets[i], 'r') as f:\n",
    "        # Extracting saved arrays for training by appropriate keys\n",
    "        # Saving them into new variables\n",
    "        x_train = f['x_train']  # HDF5 dataset\n",
    "        y_train = f['y_train']  # HDF5 dataset\n",
    "        # Converting them into Numpy arrays\n",
    "        x_train = np.array(x_train)  # Numpy arrays\n",
    "        y_train = np.array(y_train)  # Numpy arrays\n",
    "\n",
    "        # Extracting saved arrays for validation by appropriate keys\n",
    "        # Saving them into new variables\n",
    "        x_validation = f['x_validation']  # HDF5 dataset\n",
    "        y_validation = f['y_validation']  # HDF5 dataset\n",
    "        # Converting them into Numpy arrays\n",
    "        x_validation = np.array(x_validation)  # Numpy arrays\n",
    "        y_validation = np.array(y_validation)  # Numpy arrays\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Following dataset is successfully opened:        ', datasets[i])\n",
    "    \n",
    "    \n",
    "    # Preparing classes to be passed into the model\n",
    "    # Transforming them from vectors to binary matrices\n",
    "    # It is needed to set relationship between classes to be understood by the algorithm\n",
    "    # Such format is commonly used in training and predicting\n",
    "    y_train = to_categorical(y_train, num_classes = 10)\n",
    "    y_validation = to_categorical(y_validation, num_classes = 10)\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Binary matrices are successfully created:        ', datasets[i])\n",
    " \n",
    "\n",
    "    # Preparing filepath to save best weights\n",
    "    # (!) On Windows, it might need to change\n",
    "    # this: + '/' +\n",
    "    # to this: + '\\' +\n",
    "    # or to this: + '\\\\' +\n",
    "    best_weights_filepath = 'cifar10' + '/' + 'w_1' + datasets[i][7:-5] + '.h5'\n",
    "    \n",
    "    # Formatting options to save all weights for every epoch\n",
    "    # 'cifar10' + '/' + 'w_1' + datasets[i][7:-5] + '_{epoch:02d}_{val_accuracy:.4f}' + '.h5'\n",
    "    \n",
    "    # Defining schedule to save best weights\n",
    "    best_weights = ModelCheckpoint(filepath=best_weights_filepath,\n",
    "                                   save_weights_only=True,                                   \n",
    "                                   monitor='val_accuracy',\n",
    "                                   mode='max',\n",
    "                                   save_best_only=True,\n",
    "                                   period=1,\n",
    "                                   verbose=1)\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Schedule to save best weights is created:        ', datasets[i])\n",
    "\n",
    "    \n",
    "    # Checking if RGB dataset is opened\n",
    "    if i <= 1:\n",
    "        # Training RGB model with current dataset\n",
    "        temp = model_rgb[i].fit(x_train, y_train,\n",
    "                                batch_size=50,\n",
    "                                epochs=epochs,\n",
    "                                validation_data=(x_validation, y_validation),\n",
    "                                callbacks=[learning_rate, best_weights],\n",
    "                                verbose=1)\n",
    "\n",
    "        \n",
    "        # Adding results of 1st model for current RGB dataset in the list\n",
    "        h.append(temp)\n",
    "        \n",
    "        \n",
    "        # Check points\n",
    "        print('1st model for RGB is successfully trained on:    ', datasets[i])\n",
    "        print('Trained weights for RGB are saved successfully:  ', 'w_1' + datasets[i][7:-5] + '.h5')\n",
    "        print()\n",
    "    \n",
    "    # Checking if GRAY dataset is opened\n",
    "    elif i >= 2:\n",
    "        # Training GRAY model with current dataset\n",
    "        temp = model_gray[i-2].fit(x_train, y_train,\n",
    "                                   batch_size=50,\n",
    "                                   epochs=epochs,\n",
    "                                   validation_data=(x_validation, y_validation),\n",
    "                                   callbacks=[learning_rate, best_weights],\n",
    "                                   verbose=1)\n",
    "\n",
    "        \n",
    "        # Adding results of 1st model for current GRAY dataset in the list\n",
    "        h.append(temp)\n",
    "        \n",
    "        \n",
    "        # Check points\n",
    "        print('1st model for GRAY is successfully trained on:   ', datasets[i])\n",
    "        print('Trained weights for GRAY are saved successfully: ', 'w_1' + datasets[i][7:-5] + '.h5')\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10 dataset, 1st model\n",
    "\n",
    "## Step 4: Showing and plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resulted accuracies of all CIFAR-10 datasets for 1st model\n",
    "for i in range(4):\n",
    "    print('T: {0:.5f},  V: {1:.5f},  D: {2}'.format(max(h[i].history['accuracy']),\n",
    "                                                    max(h[i].history['val_accuracy']),\n",
    "                                                    datasets[i][8:-5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing other parameters that history holds\n",
    "print(h[0].params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "\n",
    "\n",
    "# Plotting accuracies of all CIFAR-10 datasets for 1st model\n",
    "plt.plot(h[0].history['val_accuracy'], '-o')\n",
    "plt.plot(h[1].history['val_accuracy'], '-o')\n",
    "plt.plot(h[2].history['val_accuracy'], '-o')\n",
    "plt.plot(h[3].history['val_accuracy'], '-o')\n",
    "\n",
    "\n",
    "# Showing legend\n",
    "plt.legend(['rgb_255_mean', 'rgb_255_mean_std',\n",
    "            'gray_255_mean', 'gray_255_mean_std'],\n",
    "           loc='lower right',\n",
    "           fontsize='xx-large')\n",
    "\n",
    "\n",
    "# Giving name to axes\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "\n",
    "\n",
    "# Setting limit along Y axis\n",
    "plt.ylim(0.71, 0.84)\n",
    "\n",
    "\n",
    "# Giving name to the plot\n",
    "plt.title('Validation accuracies of 1st model for CIFAR-10 Datasets', fontsize=16)\n",
    "\n",
    "\n",
    "# Saving plot\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "plt.savefig('cifar10' + '/' + 'validation_model_1_cifar10_dataset.png', dpi=500)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "\n",
    "\n",
    "# Plotting training and validation losses of all CIFAR-10 datasets for 1st model\n",
    "plt.plot(h[0].history['loss'], '-ob')\n",
    "plt.plot(h[1].history['loss'], '-og')\n",
    "plt.plot(h[2].history['loss'], '-or')\n",
    "plt.plot(h[3].history['loss'], '-oc')\n",
    "\n",
    "plt.plot(h[0].history['val_loss'], '-ob')\n",
    "plt.plot(h[1].history['val_loss'], '-og')\n",
    "plt.plot(h[2].history['val_loss'], '-or')\n",
    "plt.plot(h[3].history['val_loss'], '-oc')\n",
    "\n",
    "\n",
    "# Showing legend\n",
    "plt.legend(['rgb_255_mean', 'rgb_255_mean_std',\n",
    "            'gray_255_mean', 'gray_255_mean_std'],\n",
    "           loc='upper right',\n",
    "           fontsize='large')\n",
    "\n",
    "\n",
    "# Giving name to axes\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "\n",
    "\n",
    "# Giving name to the plot\n",
    "plt.title('Losses of 1st model for CIFAR-10 Datasets', fontsize=16)\n",
    "\n",
    "\n",
    "# Saving plot\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "plt.savefig('cifar10' + '/' + 'losses_model_1_cifar10_dataset.png', dpi=500)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing list of scheduled learning rate for every epoch\n",
    "print(h[0].history['lr'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Plotting scheduled learning rate\n",
    "plt.plot(h[0].history['lr'], '-mo')\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10 dataset, 2nd model\n",
    "\n",
    "## Step 1: Loading saved 2nd model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining lists to collect models in\n",
    "model_rgb = []\n",
    "model_gray = []\n",
    "\n",
    "\n",
    "# Loading 2nd model for CIFAR-10 dataset\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "for i in range(2):\n",
    "    model_rgb.append(load_model(full_path_to_Section5 + '/' + \n",
    "                                'cifar10' + '/' + \n",
    "                                'model_2_cifar10_rgb.h5'))\n",
    "    \n",
    "    model_gray.append(load_model(full_path_to_Section5 + '/' + \n",
    "                                 'cifar10' + '/' + \n",
    "                                 'model_2_cifar10_gray.h5'))\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Models are successfully loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing models' input shapes\n",
    "print(model_rgb[0].layers[0].input_shape)\n",
    "print()\n",
    "print(model_gray[0].layers[0].input_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10 dataset, 2nd model\n",
    "\n",
    "## Step 2: Setting up learning rate & epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining number of epochs\n",
    "epochs = 50\n",
    "\n",
    "\n",
    "# Defining schedule to update learning rate\n",
    "learning_rate = LearningRateScheduler(lambda x: 1e-2 * 0.95 ** (x + epochs), verbose=1)\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Number of epochs and schedule for learning rate are set successfully')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10 dataset, 2nd model\n",
    "\n",
    "## Step 3: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If you're using Nvidia GPU and 'cnngpu' environment, there might be an issue like:\n",
    "'''Failed to get convolution algorithm. This is probably because cuDNN failed to initialize'''\n",
    "# In this case, close all Jupyter Notebooks, close Terminal Window or Anaconda Prompt\n",
    "# Open again just this one Jupyter Notebook and run it\n",
    "\n",
    "\n",
    "# Preparing list with datasets' names\n",
    "datasets = ['dataset_cifar10_rgb_255_mean.hdf5',\n",
    "            'dataset_cifar10_rgb_255_mean_std.hdf5',\n",
    "            'dataset_cifar10_gray_255_mean.hdf5',\n",
    "            'dataset_cifar10_gray_255_mean_std.hdf5']\n",
    "\n",
    "\n",
    "# Defining list to collect results in\n",
    "h = []\n",
    "\n",
    "\n",
    "# Training 2nd model with all CIFAR-10 datasets in a loop\n",
    "for i in range(4):\n",
    "    # Opening saved CIFAR-10 dataset from HDF5 binary file\n",
    "    # Initiating File object\n",
    "    # Opening file in reading mode by 'r'\n",
    "    # (!) On Windows, it might need to change\n",
    "    # this: + '/' +\n",
    "    # to this: + '\\' +\n",
    "    # or to this: + '\\\\' +\n",
    "    with h5py.File(full_path_to_Section4 + '/' + 'cifar10' + '/' + datasets[i], 'r') as f:\n",
    "        # Extracting saved arrays for training by appropriate keys\n",
    "        # Saving them into new variables\n",
    "        x_train = f['x_train']  # HDF5 dataset\n",
    "        y_train = f['y_train']  # HDF5 dataset\n",
    "        # Converting them into Numpy arrays\n",
    "        x_train = np.array(x_train)  # Numpy arrays\n",
    "        y_train = np.array(y_train)  # Numpy arrays\n",
    "\n",
    "        # Extracting saved arrays for validation by appropriate keys\n",
    "        # Saving them into new variables\n",
    "        x_validation = f['x_validation']  # HDF5 dataset\n",
    "        y_validation = f['y_validation']  # HDF5 dataset\n",
    "        # Converting them into Numpy arrays\n",
    "        x_validation = np.array(x_validation)  # Numpy arrays\n",
    "        y_validation = np.array(y_validation)  # Numpy arrays\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Following dataset is successfully opened:        ', datasets[i])\n",
    "    \n",
    "    \n",
    "    # Preparing classes to be passed into the model\n",
    "    # Transforming them from vectors to binary matrices\n",
    "    # It is needed to set relationship between classes to be understood by the algorithm\n",
    "    # Such format is commonly used in training and predicting\n",
    "    y_train = to_categorical(y_train, num_classes = 10)\n",
    "    y_validation = to_categorical(y_validation, num_classes = 10)\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Binary matrices are successfully created:        ', datasets[i])\n",
    "\n",
    "\n",
    "    # Preparing filepath to save best weights\n",
    "    # (!) On Windows, it might need to change\n",
    "    # this: + '/' +\n",
    "    # to this: + '\\' +\n",
    "    # or to this: + '\\\\' +\n",
    "    best_weights_filepath = 'cifar10' + '/' + 'w_2' + datasets[i][7:-5] + '.h5'\n",
    "    \n",
    "    # Formatting options to save all weights for every epoch\n",
    "    # 'cifar10' + '/' + 'w_2' + datasets[i][7:-5] + '_{epoch:02d}_{val_accuracy:.4f}' + '.h5'\n",
    "    \n",
    "    # Defining schedule to save best weights\n",
    "    best_weights = ModelCheckpoint(filepath=best_weights_filepath,\n",
    "                                   save_weights_only=True,                                   \n",
    "                                   monitor='val_accuracy',\n",
    "                                   mode='max',\n",
    "                                   save_best_only=True,\n",
    "                                   period=1,\n",
    "                                   verbose=1)\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Schedule to save best weights is created:        ', datasets[i])\n",
    "    \n",
    "    \n",
    "    # Defining schedule to save intermediate weights\n",
    "    class CustomCallback(Callback):\n",
    "        # Constructor of the class\n",
    "        def __init__(self):\n",
    "            # Defining variable to be as a part of filename\n",
    "            self.filename = 0\n",
    "        \n",
    "        # Function that is called at the end of every batch\n",
    "        def on_train_batch_end(self, batch, logs=None):\n",
    "            # Checking if it is every 100th batch\n",
    "            if batch % 100 == 0:\n",
    "                # Preparing filepath to save intermediate weights\n",
    "                # (!) On Windows, it might need to change\n",
    "                # this: + '/' +\n",
    "                # to this: + '\\' +\n",
    "                # or to this: + '\\\\' +\n",
    "                intermediate_weights_filepath = 'cifar10' + '/' + \\\n",
    "                                                'intermediate' + '/' + \\\n",
    "                                                '{0:04d}'.format(self.filename) + \\\n",
    "                                                '_w_2' + \\\n",
    "                                                datasets[i][7:-5] + '.h5'\n",
    "                \n",
    "                # Getting weights only for the first convolutional layer\n",
    "                weights_layer_0 = self.model.get_weights()[0]\n",
    "                \n",
    "                # Saving obtained weights into new HDF5 binary file\n",
    "                # Initiating File object\n",
    "                # Creating file with current name\n",
    "                # Opening it in writing mode by 'w'\n",
    "                # (!) On Windows, it might need to change\n",
    "                # this: + '/' +\n",
    "                # to this: + '\\' +\n",
    "                # or to this: + '\\\\' +\n",
    "                with h5py.File(intermediate_weights_filepath, 'w') as f:\n",
    "                    # Calling method to create dataset of given shape and type\n",
    "                    # Saving Numpy array with weights from the first layer\n",
    "                    f.create_dataset('weights_layer_0', data=weights_layer_0, dtype='f')\n",
    "                    \n",
    "                # Increasing variable to be as a part of the next filename\n",
    "                self.filename += 1\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Schedule to save intermediate weights is created:', datasets[i])\n",
    "\n",
    "    \n",
    "    # Checking if RGB dataset is opened\n",
    "    if i <= 1:\n",
    "        # Checking if second RGB dataset is opened\n",
    "        if i == 1:\n",
    "            # Training RGB model with current dataset\n",
    "            temp = model_rgb[i].fit(x_train, y_train,\n",
    "                                    batch_size=50,\n",
    "                                    epochs=epochs,\n",
    "                                    validation_data=(x_validation, y_validation),\n",
    "                                    callbacks=[learning_rate, best_weights, CustomCallback()],\n",
    "                                    verbose=1)\n",
    "        \n",
    "        # Checking if first RGB dataset is opened\n",
    "        else:\n",
    "            # Training RGB model with current dataset\n",
    "            temp = model_rgb[i].fit(x_train, y_train,\n",
    "                                    batch_size=50,\n",
    "                                    epochs=epochs,\n",
    "                                    validation_data=(x_validation, y_validation),\n",
    "                                    callbacks=[learning_rate, best_weights],\n",
    "                                    verbose=1)\n",
    "\n",
    "        \n",
    "        # Adding results of 2nd model for current RGB dataset in the list\n",
    "        h.append(temp)\n",
    "        \n",
    "        \n",
    "        # Check points\n",
    "        print('2nd model for RGB is successfully trained on:    ', datasets[i])\n",
    "        print('Trained weights for RGB are saved successfully:  ', 'w_2' + datasets[i][7:-5] + '.h5')\n",
    "        print()\n",
    "    \n",
    "    # Checking if GRAY dataset is opened\n",
    "    elif i >= 2:\n",
    "        # Training GRAY model with current dataset\n",
    "        temp = model_gray[i-2].fit(x_train, y_train,\n",
    "                                   batch_size=50,\n",
    "                                   epochs=epochs,\n",
    "                                   validation_data=(x_validation, y_validation),\n",
    "                                   callbacks=[learning_rate, best_weights],\n",
    "                                   verbose=1)\n",
    "\n",
    "        \n",
    "        # Adding results of 2nd model for current GRAY dataset in the list\n",
    "        h.append(temp)\n",
    "        \n",
    "        \n",
    "        # Check points\n",
    "        print('2nd model for GRAY is successfully trained on:   ', datasets[i])\n",
    "        print('Trained weights for GRAY are saved successfully: ', 'w_2' + datasets[i][7:-5] + '.h5')\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10 dataset, 2nd model\n",
    "\n",
    "## Step 4: Showing and plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resulted accuracies of all CIFAR-10 datasets for 2nd model\n",
    "for i in range(4):\n",
    "    print('T: {0:.5f},  V: {1:.5f},  D: {2}'.format(max(h[i].history['accuracy']),\n",
    "                                                    max(h[i].history['val_accuracy']),\n",
    "                                                    datasets[i][8:-5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing other parameters that history holds\n",
    "print(h[0].params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "\n",
    "\n",
    "# Plotting accuracies of all CIFAR-10 datasets for 2nd model\n",
    "plt.plot(h[0].history['val_accuracy'], '-o')\n",
    "plt.plot(h[1].history['val_accuracy'], '-o')\n",
    "plt.plot(h[2].history['val_accuracy'], '-o')\n",
    "plt.plot(h[3].history['val_accuracy'], '-o')\n",
    "\n",
    "\n",
    "# Showing legend\n",
    "plt.legend(['rgb_255_mean', 'rgb_255_mean_std',\n",
    "            'gray_255_mean', 'gray_255_mean_std'],\n",
    "           loc='lower right',\n",
    "           fontsize='xx-large')\n",
    "\n",
    "\n",
    "# Giving name to axes\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "\n",
    "\n",
    "# Setting limit along Y axis\n",
    "plt.ylim(0.5, 0.72)\n",
    "\n",
    "\n",
    "# Giving name to the plot\n",
    "plt.title('Validation accuracies of 2nd model for CIFAR-10 Datasets', fontsize=16)\n",
    "\n",
    "\n",
    "# Saving plot\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "plt.savefig('cifar10' + '/' + 'validation_model_2_cifar10_dataset.png', dpi=500)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "\n",
    "\n",
    "# Plotting training and validation losses of all CIFAR-10 datasets for 2nd model\n",
    "plt.plot(h[0].history['loss'], '-ob')\n",
    "plt.plot(h[1].history['loss'], '-og')\n",
    "plt.plot(h[2].history['loss'], '-or')\n",
    "plt.plot(h[3].history['loss'], '-oc')\n",
    "\n",
    "plt.plot(h[0].history['val_loss'], '-ob')\n",
    "plt.plot(h[1].history['val_loss'], '-og')\n",
    "plt.plot(h[2].history['val_loss'], '-or')\n",
    "plt.plot(h[3].history['val_loss'], '-oc')\n",
    "\n",
    "\n",
    "# Showing legend\n",
    "plt.legend(['rgb_255_mean', 'rgb_255_mean_std',\n",
    "            'gray_255_mean', 'gray_255_mean_std'],\n",
    "           loc='upper right',\n",
    "           fontsize='medium')\n",
    "\n",
    "\n",
    "# Giving name to axes\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "\n",
    "\n",
    "# Giving name to the plot\n",
    "plt.title('Losses of 2nd model for CIFAR-10 Datasets', fontsize=16)\n",
    "\n",
    "\n",
    "# Saving plot\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "plt.savefig('cifar10' + '/' + 'losses_model_2_cifar10_dataset.png', dpi=500)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing list of scheduled learning rate for every epoch\n",
    "print(h[0].history['lr'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Plotting scheduled learning rate\n",
    "plt.plot(h[0].history['lr'], '-mo')\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST dataset, 1st model\n",
    "\n",
    "## Step 1: Loading saved 1st model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining list to collect models in\n",
    "model_gray = []\n",
    "\n",
    "\n",
    "# Loading 1st model for MNIST dataset\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "for i in range(2):\n",
    "    model_gray.append(load_model(full_path_to_Section5 + '/' + \n",
    "                                 'mnist' + '/' + 'model_1_mnist_gray.h5'))\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Models are successfully loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing model's input shape\n",
    "print(model_gray[0].layers[0].input_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST dataset, 1st model\n",
    "\n",
    "## Step 2: Setting up learning rate & epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining number of epochs\n",
    "epochs = 50\n",
    "\n",
    "\n",
    "# Defining schedule to update learning rate\n",
    "learning_rate = LearningRateScheduler(lambda x: 1e-2 * 0.95 ** (x + epochs), verbose=1)\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Number of epochs and schedule for learning rate are set successfully')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST dataset, 1st model\n",
    "\n",
    "## Step 3: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If you're using Nvidia GPU and 'cnngpu' environment, there might be an issue like:\n",
    "'''Failed to get convolution algorithm. This is probably because cuDNN failed to initialize'''\n",
    "# In this case, close all Jupyter Notebooks, close Terminal Window or Anaconda Prompt\n",
    "# Open again just this one Jupyter Notebook and run it\n",
    "\n",
    "\n",
    "# Preparing list with datasets' names\n",
    "datasets = ['dataset_mnist_gray_255_mean.hdf5',\n",
    "            'dataset_mnist_gray_255_mean_std.hdf5']\n",
    "\n",
    "\n",
    "# Defining list to collect results in\n",
    "h = []\n",
    "\n",
    "\n",
    "# Training 1st model with all MNIST datasets in a loop\n",
    "for i in range(2):    \n",
    "    # Opening saved MNIST dataset from HDF5 binary file\n",
    "    # Initiating File object\n",
    "    # Opening file in reading mode by 'r'\n",
    "    # (!) On Windows, it might need to change\n",
    "    # this: + '/' +\n",
    "    # to this: + '\\' +\n",
    "    # or to this: + '\\\\' +\n",
    "    with h5py.File(full_path_to_Section4 + '/' + 'mnist' + '/' + datasets[i], 'r') as f:\n",
    "        # Extracting saved arrays for training by appropriate keys\n",
    "        # Saving them into new variables\n",
    "        x_train = f['x_train']  # HDF5 dataset\n",
    "        y_train = f['y_train']  # HDF5 dataset\n",
    "        # Converting them into Numpy arrays\n",
    "        x_train = np.array(x_train)  # Numpy arrays\n",
    "        y_train = np.array(y_train)  # Numpy arrays\n",
    "\n",
    "        # Extracting saved arrays for validation by appropriate keys\n",
    "        # Saving them into new variables\n",
    "        x_validation = f['x_validation']  # HDF5 dataset\n",
    "        y_validation = f['y_validation']  # HDF5 dataset\n",
    "        # Converting them into Numpy arrays\n",
    "        x_validation = np.array(x_validation)  # Numpy arrays\n",
    "        y_validation = np.array(y_validation)  # Numpy arrays\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Following dataset is successfully opened:        ', datasets[i])\n",
    "    \n",
    "    \n",
    "    # Preparing classes to be passed into the model\n",
    "    # Transforming them from vectors to binary matrices\n",
    "    # It is needed to set relationship between classes to be understood by the algorithm\n",
    "    # Such format is commonly used in training and predicting\n",
    "    y_train = to_categorical(y_train, num_classes = 10)\n",
    "    y_validation = to_categorical(y_validation, num_classes = 10)\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Binary matrices are successfully created:        ', datasets[i])\n",
    " \n",
    "\n",
    "    # Preparing filepath to save best weights\n",
    "    # (!) On Windows, it might need to change\n",
    "    # this: + '/' +\n",
    "    # to this: + '\\' +\n",
    "    # or to this: + '\\\\' +\n",
    "    best_weights_filepath = 'mnist' + '/' + 'w_1' + datasets[i][7:-5] + '.h5'\n",
    "    \n",
    "    # Formatting options to save all weights for every epoch\n",
    "    # 'mnist' + '/' + 'w_1' + datasets[i][7:-5] + '_{epoch:02d}_{val_accuracy:.4f}' + '.h5'\n",
    "    \n",
    "    # Defining schedule to save best weights\n",
    "    best_weights = ModelCheckpoint(filepath=best_weights_filepath,\n",
    "                                   save_weights_only=True,                                   \n",
    "                                   monitor='val_accuracy',\n",
    "                                   mode='max',\n",
    "                                   save_best_only=True,\n",
    "                                   period=1,\n",
    "                                   verbose=1)\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Schedule to save best weights is created:        ', datasets[i])\n",
    "\n",
    "    \n",
    "    # Training GRAY model with current dataset\n",
    "    temp = model_gray[i].fit(x_train, y_train,\n",
    "                             batch_size=50,\n",
    "                             epochs=epochs,\n",
    "                             validation_data=(x_validation, y_validation),\n",
    "                             callbacks=[learning_rate, best_weights],\n",
    "                             verbose=1)\n",
    "\n",
    "    \n",
    "    # Adding results of 1st model for current GRAY dataset in the list\n",
    "    h.append(temp)\n",
    "    \n",
    "    \n",
    "    # Check points\n",
    "    print('1st model for GRAY is successfully trained on:   ', datasets[i])\n",
    "    print('Trained weights for GRAY are saved successfully: ', 'w_1' + datasets[i][7:-5] + '.h5')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST dataset, 1st model\n",
    "\n",
    "## Step 4: Showing and plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resulted accuracies of all MNIST datasets for 1st model\n",
    "for i in range(2):\n",
    "    print('T: {0:.5f},  V: {1:.5f},  D: {2}'.format(max(h[i].history['accuracy']),\n",
    "                                                    max(h[i].history['val_accuracy']),\n",
    "                                                    datasets[i][8:-5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing other parameters that history holds\n",
    "print(h[0].params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "\n",
    "\n",
    "# Plotting accuracies of all MNIST datasets for 1st model\n",
    "plt.plot(h[0].history['val_accuracy'], '-o')\n",
    "plt.plot(h[1].history['val_accuracy'], '-o')\n",
    "\n",
    "\n",
    "# Showing legend\n",
    "plt.legend(['gray_255_mean', 'gray_255_mean_std'],\n",
    "           loc='lower right',\n",
    "           fontsize='xx-large')\n",
    "\n",
    "\n",
    "# Giving name to axes\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "\n",
    "\n",
    "# Setting limit along Y axis\n",
    "plt.ylim(0.989, 0.995)\n",
    "\n",
    "\n",
    "# Giving name to the plot\n",
    "plt.title('Validation accuracies of 1st model for MNIST Datasets', fontsize=16)\n",
    "\n",
    "\n",
    "# Saving plot\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "plt.savefig('mnist' + '/' + 'validation_model_1_mnist_dataset.png', dpi=500)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "\n",
    "\n",
    "# Plotting training and validation losses of all MNIST datasets for 1st model\n",
    "plt.plot(h[0].history['loss'], '-ob')\n",
    "plt.plot(h[1].history['loss'], '-og')\n",
    "\n",
    "plt.plot(h[0].history['val_loss'], '-ob')\n",
    "plt.plot(h[1].history['val_loss'], '-og')\n",
    "\n",
    "\n",
    "# Showing legend\n",
    "plt.legend(['gray_255_mean', 'gray_255_mean_std'],\n",
    "           loc='upper right',\n",
    "           fontsize='large')\n",
    "\n",
    "\n",
    "# Giving name to axes\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "\n",
    "\n",
    "# Giving name to the plot\n",
    "plt.title('Losses of 1st model for MNIST Datasets', fontsize=16)\n",
    "\n",
    "\n",
    "# Saving plot\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "plt.savefig('mnist' + '/' + 'losses_model_1_mnist_dataset.png', dpi=500)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing list of scheduled learning rate for every epoch\n",
    "print(h[0].history['lr'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Plotting scheduled learning rate\n",
    "plt.plot(h[0].history['lr'], '-mo')\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST dataset, 2nd model\n",
    "\n",
    "## Step 1: Loading saved 2nd model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining lists to collect models in\n",
    "model_gray = []\n",
    "\n",
    "\n",
    "# Loading 2nd model for MNIST dataset\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "for i in range(2):\n",
    "    model_gray.append(load_model(full_path_to_Section5 + '/' + \n",
    "                                 'mnist' + '/' + \n",
    "                                 'model_2_mnist_gray.h5'))\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Models are successfully loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing model's input shape\n",
    "print(model_gray[0].layers[0].input_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST dataset, 2nd model\n",
    "\n",
    "## Step 2: Setting up learning rate & epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining number of epochs\n",
    "epochs = 50\n",
    "\n",
    "\n",
    "# Defining schedule to update learning rate\n",
    "learning_rate = LearningRateScheduler(lambda x: 1e-2 * 0.95 ** (x + epochs), verbose=1)\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Number of epochs and schedule for learning rate are set successfully')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST dataset, 2nd model\n",
    "\n",
    "## Step 3: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If you're using Nvidia GPU and 'cnngpu' environment, there might be an issue like:\n",
    "'''Failed to get convolution algorithm. This is probably because cuDNN failed to initialize'''\n",
    "# In this case, close all Jupyter Notebooks, close Terminal Window or Anaconda Prompt\n",
    "# Open again just this one Jupyter Notebook and run it\n",
    "\n",
    "\n",
    "# Preparing list with datasets' names\n",
    "datasets = ['dataset_mnist_gray_255_mean.hdf5',\n",
    "            'dataset_mnist_gray_255_mean_std.hdf5']\n",
    "\n",
    "\n",
    "# Defining list to collect results in\n",
    "h = []\n",
    "\n",
    "\n",
    "# Training 2nd model with all MNIST datasets in a loop\n",
    "for i in range(2):\n",
    "    # Opening saved MNIST dataset from HDF5 binary file\n",
    "    # Initiating File object\n",
    "    # Opening file in reading mode by 'r'\n",
    "    # (!) On Windows, it might need to change\n",
    "    # this: + '/' +\n",
    "    # to this: + '\\' +\n",
    "    # or to this: + '\\\\' +\n",
    "    with h5py.File(full_path_to_Section4 + '/' + 'mnist' + '/' + datasets[i], 'r') as f:\n",
    "        # Extracting saved arrays for training by appropriate keys\n",
    "        # Saving them into new variables\n",
    "        x_train = f['x_train']  # HDF5 dataset\n",
    "        y_train = f['y_train']  # HDF5 dataset\n",
    "        # Converting them into Numpy arrays\n",
    "        x_train = np.array(x_train)  # Numpy arrays\n",
    "        y_train = np.array(y_train)  # Numpy arrays\n",
    "\n",
    "        # Extracting saved arrays for validation by appropriate keys\n",
    "        # Saving them into new variables\n",
    "        x_validation = f['x_validation']  # HDF5 dataset\n",
    "        y_validation = f['y_validation']  # HDF5 dataset\n",
    "        # Converting them into Numpy arrays\n",
    "        x_validation = np.array(x_validation)  # Numpy arrays\n",
    "        y_validation = np.array(y_validation)  # Numpy arrays\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Following dataset is successfully opened:        ', datasets[i])\n",
    "    \n",
    "    \n",
    "    # Preparing classes to be passed into the model\n",
    "    # Transforming them from vectors to binary matrices\n",
    "    # It is needed to set relationship between classes to be understood by the algorithm\n",
    "    # Such format is commonly used in training and predicting\n",
    "    y_train = to_categorical(y_train, num_classes = 10)\n",
    "    y_validation = to_categorical(y_validation, num_classes = 10)\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Binary matrices are successfully created:        ', datasets[i])\n",
    "\n",
    "\n",
    "    # Preparing filepath to save best weights\n",
    "    # (!) On Windows, it might need to change\n",
    "    # this: + '/' +\n",
    "    # to this: + '\\' +\n",
    "    # or to this: + '\\\\' +\n",
    "    best_weights_filepath = 'mnist' + '/' + 'w_2' + datasets[i][7:-5] + '.h5'\n",
    "    \n",
    "    # Formatting options to save all weights for every epoch\n",
    "    # 'mnist' + '/' + 'w_2' + datasets[i][7:-5] + '_{epoch:02d}_{val_accuracy:.4f}' + '.h5'\n",
    "    \n",
    "    # Defining schedule to save best weights\n",
    "    best_weights = ModelCheckpoint(filepath=best_weights_filepath,\n",
    "                                   save_weights_only=True,                                   \n",
    "                                   monitor='val_accuracy',\n",
    "                                   mode='max',\n",
    "                                   save_best_only=True,\n",
    "                                   period=1,\n",
    "                                   verbose=1)\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Schedule to save best weights is created:        ', datasets[i])\n",
    "    \n",
    "    \n",
    "    # Defining schedule to save intermediate weights\n",
    "    class CustomCallback(Callback):\n",
    "        # Constructor of the class\n",
    "        def __init__(self):\n",
    "            # Defining variable to be as a part of filename\n",
    "            self.filename = 0\n",
    "        \n",
    "        # Function that is called at the end of every batch\n",
    "        def on_train_batch_end(self, batch, logs=None):\n",
    "            # Checking if it is every 100th batch\n",
    "            if batch % 100 == 0:\n",
    "                # Preparing filepath to save intermediate weights\n",
    "                # (!) On Windows, it might need to change\n",
    "                # this: + '/' +\n",
    "                # to this: + '\\' +\n",
    "                # or to this: + '\\\\' +\n",
    "                intermediate_weights_filepath = 'mnist' + '/' + \\\n",
    "                                                'intermediate' + '/' + \\\n",
    "                                                '{0:04d}'.format(self.filename) + \\\n",
    "                                                '_w_2' + \\\n",
    "                                                datasets[i][7:-5] + '.h5'\n",
    "                \n",
    "                # Getting weights only for the first convolutional layer\n",
    "                weights_layer_0 = self.model.get_weights()[0]\n",
    "                \n",
    "                # Saving obtained weights into new HDF5 binary file\n",
    "                # Initiating File object\n",
    "                # Creating file with current name\n",
    "                # Opening it in writing mode by 'w'\n",
    "                # (!) On Windows, it might need to change\n",
    "                # this: + '/' +\n",
    "                # to this: + '\\' +\n",
    "                # or to this: + '\\\\' +\n",
    "                with h5py.File(intermediate_weights_filepath, 'w') as f:\n",
    "                    # Calling method to create dataset of given shape and type\n",
    "                    # Saving Numpy array with weights from the first layer\n",
    "                    f.create_dataset('weights_layer_0', data=weights_layer_0, dtype='f')\n",
    "                    \n",
    "                # Increasing variable to be as a part of the next filename\n",
    "                self.filename += 1\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Schedule to save intermediate weights is created:', datasets[i])\n",
    "\n",
    "    \n",
    "    # Checking if second GRAY dataset is opened\n",
    "    if i == 1:\n",
    "        # Training GRAY model with current dataset\n",
    "        temp = model_gray[i].fit(x_train, y_train,\n",
    "                                 batch_size=50,\n",
    "                                 epochs=epochs,\n",
    "                                 validation_data=(x_validation, y_validation),\n",
    "                                 callbacks=[learning_rate, best_weights, CustomCallback()],\n",
    "                                 verbose=1)\n",
    "        \n",
    "    # Checking if first GRAY dataset is opened\n",
    "    else:\n",
    "        # Training GRAY model with current dataset\n",
    "        temp = model_gray[i].fit(x_train, y_train,\n",
    "                                 batch_size=50,\n",
    "                                 epochs=epochs,\n",
    "                                 validation_data=(x_validation, y_validation),\n",
    "                                 callbacks=[learning_rate, best_weights],\n",
    "                                 verbose=1)\n",
    "\n",
    "        \n",
    "    # Adding results of 2nd model for current GRAY dataset in the list\n",
    "    h.append(temp)\n",
    "    \n",
    "    \n",
    "    # Check points\n",
    "    print('2nd model for GRAY is successfully trained on:   ', datasets[i])\n",
    "    print('Trained weights for GRAY are saved successfully: ', 'w_2' + datasets[i][7:-5] + '.h5')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST dataset, 2nd model\n",
    "\n",
    "## Step 4: Showing and plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resulted accuracies of all MNIST datasets for 2nd model\n",
    "for i in range(2):\n",
    "    print('T: {0:.5f},  V: {1:.5f},  D: {2}'.format(max(h[i].history['accuracy']),\n",
    "                                                    max(h[i].history['val_accuracy']),\n",
    "                                                    datasets[i][8:-5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing other parameters that history holds\n",
    "print(h[0].params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "\n",
    "\n",
    "# Plotting accuracies of all MNIST datasets for 2nd model\n",
    "plt.plot(h[0].history['val_accuracy'], '-o')\n",
    "plt.plot(h[1].history['val_accuracy'], '-o')\n",
    "\n",
    "\n",
    "# Showing legend\n",
    "plt.legend(['gray_255_mean', 'gray_255_mean_std'],\n",
    "           loc='lower right',\n",
    "           fontsize='xx-large')\n",
    "\n",
    "\n",
    "# Giving name to axes\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "\n",
    "\n",
    "# Setting limit along Y axis\n",
    "plt.ylim(0.985, 0.993)\n",
    "\n",
    "\n",
    "# Giving name to the plot\n",
    "plt.title('Validation accuracies of 2nd model for MNIST Datasets', fontsize=16)\n",
    "\n",
    "\n",
    "# Saving plot\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "plt.savefig('mnist' + '/' + 'validation_model_2_mnist_dataset.png', dpi=500)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "\n",
    "\n",
    "# Plotting training and validation losses of all MNIST datasets for 2nd model\n",
    "plt.plot(h[0].history['loss'], '-ob')\n",
    "plt.plot(h[1].history['loss'], '-og')\n",
    "\n",
    "plt.plot(h[0].history['val_loss'], '-ob')\n",
    "plt.plot(h[1].history['val_loss'], '-og')\n",
    "\n",
    "\n",
    "# Showing legend\n",
    "plt.legend(['gray_255_mean', 'gray_255_mean_std'],\n",
    "           loc='center right',\n",
    "           fontsize='large')\n",
    "\n",
    "\n",
    "# Giving name to axes\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "\n",
    "\n",
    "# Giving name to the plot\n",
    "plt.title('Losses of 2nd model for MNIST Datasets', fontsize=16)\n",
    "\n",
    "\n",
    "# Saving plot\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "plt.savefig('mnist' + '/' + 'losses_model_2_mnist_dataset.png', dpi=500)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing list of scheduled learning rate for every epoch\n",
    "print(h[0].history['lr'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Plotting scheduled learning rate\n",
    "plt.plot(h[0].history['lr'], '-mo')\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traffic Signs dataset, 1st model\n",
    "\n",
    "## Step 1: Loading saved 1st model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining lists to collect models in\n",
    "model_rgb = []\n",
    "model_gray = []\n",
    "\n",
    "\n",
    "# Loading 1st model for Traffic Signs dataset\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "for i in range(2):\n",
    "    model_rgb.append(load_model(full_path_to_Section5 + '/' + \n",
    "                                'ts' + '/' + \n",
    "                                'model_1_ts_rgb.h5'))\n",
    "    \n",
    "    model_gray.append(load_model(full_path_to_Section5 + '/' + \n",
    "                                 'ts' + '/' + \n",
    "                                 'model_1_ts_gray.h5'))\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Models are successfully loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing models' input shapes\n",
    "print(model_rgb[0].layers[0].input_shape)\n",
    "print()\n",
    "print(model_gray[0].layers[0].input_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traffic Signs dataset, 1st model\n",
    "\n",
    "## Step 2: Setting up learning rate & epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining number of epochs\n",
    "epochs = 50\n",
    "\n",
    "\n",
    "# Defining schedule to update learning rate\n",
    "learning_rate = LearningRateScheduler(lambda x: 1e-2 * 0.95 ** (x + epochs), verbose=1)\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Number of epochs and schedule for learning rate are set successfully')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traffic Signs dataset, 1st model\n",
    "\n",
    "## Step 3: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If you're using Nvidia GPU and 'cnngpu' environment, there might be an issue like:\n",
    "'''Failed to get convolution algorithm. This is probably because cuDNN failed to initialize'''\n",
    "# In this case, close all Jupyter Notebooks, close Terminal Window or Anaconda Prompt\n",
    "# Open again just this one Jupyter Notebook and run it\n",
    "\n",
    "\n",
    "# Preparing list with datasets' names\n",
    "datasets = ['dataset_ts_rgb_255_mean.hdf5',\n",
    "            'dataset_ts_rgb_255_mean_std.hdf5',\n",
    "            'dataset_ts_gray_255_mean.hdf5',\n",
    "            'dataset_ts_gray_255_mean_std.hdf5']\n",
    "\n",
    "\n",
    "# Defining list to collect results in\n",
    "h = []\n",
    "\n",
    "\n",
    "# Training 1st model with all Traffic Signs datasets in a loop\n",
    "for i in range(4):\n",
    "    # Opening saved Traffic Signs dataset from HDF5 binary file\n",
    "    # Initiating File object\n",
    "    # Opening file in reading mode by 'r'\n",
    "    # (!) On Windows, it might need to change\n",
    "    # this: + '/' +\n",
    "    # to this: + '\\' +\n",
    "    # or to this: + '\\\\' +\n",
    "    with h5py.File(full_path_to_Section4 + '/' + 'ts' + '/' + datasets[i], 'r') as f:\n",
    "        # Extracting saved arrays for training by appropriate keys\n",
    "        # Saving them into new variables\n",
    "        x_train = f['x_train']  # HDF5 dataset\n",
    "        y_train = f['y_train']  # HDF5 dataset\n",
    "        # Converting them into Numpy arrays\n",
    "        x_train = np.array(x_train)  # Numpy arrays\n",
    "        y_train = np.array(y_train)  # Numpy arrays\n",
    "\n",
    "        # Extracting saved arrays for validation by appropriate keys\n",
    "        # Saving them into new variables\n",
    "        x_validation = f['x_validation']  # HDF5 dataset\n",
    "        y_validation = f['y_validation']  # HDF5 dataset\n",
    "        # Converting them into Numpy arrays\n",
    "        x_validation = np.array(x_validation)  # Numpy arrays\n",
    "        y_validation = np.array(y_validation)  # Numpy arrays\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Following dataset is successfully opened:        ', datasets[i])\n",
    "    \n",
    "    \n",
    "    # Preparing classes to be passed into the model\n",
    "    # Transforming them from vectors to binary matrices\n",
    "    # It is needed to set relationship between classes to be understood by the algorithm\n",
    "    # Such format is commonly used in training and predicting\n",
    "    y_train = to_categorical(y_train, num_classes = 43)\n",
    "    y_validation = to_categorical(y_validation, num_classes = 43)\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Binary matrices are successfully created:        ', datasets[i])\n",
    " \n",
    "\n",
    "    # Preparing filepath to save best weights\n",
    "    # (!) On Windows, it might need to change\n",
    "    # this: + '/' +\n",
    "    # to this: + '\\' +\n",
    "    # or to this: + '\\\\' +\n",
    "    best_weights_filepath = 'ts' + '/' + 'w_1' + datasets[i][7:-5] + '.h5'\n",
    "    \n",
    "    # Formatting options to save all weights for every epoch\n",
    "    # 'ts' + '/' + 'w_1' + datasets[i][7:-5] + '_{epoch:02d}_{val_accuracy:.4f}' + '.h5'\n",
    "    \n",
    "    # Defining schedule to save best weights\n",
    "    best_weights = ModelCheckpoint(filepath=best_weights_filepath,\n",
    "                                   save_weights_only=True,                                   \n",
    "                                   monitor='val_accuracy',\n",
    "                                   mode='max',\n",
    "                                   save_best_only=True,\n",
    "                                   period=1,\n",
    "                                   verbose=1)\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Schedule to save best weights is created:        ', datasets[i])\n",
    "\n",
    "    \n",
    "    # Checking if RGB dataset is opened\n",
    "    if i <= 1:\n",
    "        # Training RGB model with current dataset\n",
    "        temp = model_rgb[i].fit(x_train, y_train,\n",
    "                                batch_size=50,\n",
    "                                epochs=epochs,\n",
    "                                validation_data=(x_validation, y_validation),\n",
    "                                callbacks=[learning_rate, best_weights],\n",
    "                                verbose=1)\n",
    "\n",
    "        \n",
    "        # Adding results of 1st model for current RGB dataset in the list\n",
    "        h.append(temp)\n",
    "        \n",
    "        \n",
    "        # Check points\n",
    "        print('1st model for RGB is successfully trained on:    ', datasets[i])\n",
    "        print('Trained weights for RGB are saved successfully:  ', 'w_1' + datasets[i][7:-5] + '.h5')\n",
    "        print()\n",
    "    \n",
    "    # Checking if GRAY dataset is opened\n",
    "    elif i >= 2:\n",
    "        # Training GRAY model with current dataset\n",
    "        temp = model_gray[i-2].fit(x_train, y_train,\n",
    "                                   batch_size=50,\n",
    "                                   epochs=epochs,\n",
    "                                   validation_data=(x_validation, y_validation),\n",
    "                                   callbacks=[learning_rate, best_weights],\n",
    "                                   verbose=1)\n",
    "\n",
    "        \n",
    "        # Adding results of 1st model for current GRAY dataset in the list\n",
    "        h.append(temp)\n",
    "        \n",
    "        \n",
    "        # Check points\n",
    "        print('1st model for GRAY is successfully trained on:   ', datasets[i])\n",
    "        print('Trained weights for GRAY are saved successfully: ', 'w_1' + datasets[i][7:-5] + '.h5')\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traffic Signs dataset, 1st model\n",
    "\n",
    "## Step 4: Showing and plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resulted accuracies of all Traffic Signs datasets for 1st model\n",
    "for i in range(4):\n",
    "    print('T: {0:.5f},  V: {1:.5f},  D: {2}'.format(max(h[i].history['accuracy']),\n",
    "                                                    max(h[i].history['val_accuracy']),\n",
    "                                                    datasets[i][8:-5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing other parameters that history holds\n",
    "print(h[0].params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "\n",
    "\n",
    "# Plotting accuracies of all Traffic Signs datasets for 1st model\n",
    "plt.plot(h[0].history['val_accuracy'], '-o')\n",
    "plt.plot(h[1].history['val_accuracy'], '-o')\n",
    "plt.plot(h[2].history['val_accuracy'], '-o')\n",
    "plt.plot(h[3].history['val_accuracy'], '-o')\n",
    "\n",
    "\n",
    "# Showing legend\n",
    "plt.legend(['rgb_255_mean', 'rgb_255_mean_std',\n",
    "            'gray_255_mean', 'gray_255_mean_std'],\n",
    "           loc='lower right',\n",
    "           fontsize='xx-large')\n",
    "\n",
    "\n",
    "# Giving name to axes\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "\n",
    "\n",
    "# Setting limit along Y axis\n",
    "plt.ylim(0.97, 0.9992)\n",
    "\n",
    "\n",
    "# Giving name to the plot\n",
    "plt.title('Validation accuracies of 1st model for Traffic Signs Datasets', fontsize=16)\n",
    "\n",
    "\n",
    "# Saving plot\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "plt.savefig('ts' + '/' + 'validation_model_1_ts_dataset.png', dpi=500)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "\n",
    "\n",
    "# Plotting training and validation losses of all Traffic Signs datasets for 1st model\n",
    "plt.plot(h[0].history['loss'], '-ob')\n",
    "plt.plot(h[1].history['loss'], '-og')\n",
    "plt.plot(h[2].history['loss'], '-or')\n",
    "plt.plot(h[3].history['loss'], '-oc')\n",
    "\n",
    "plt.plot(h[0].history['val_loss'], '-ob')\n",
    "plt.plot(h[1].history['val_loss'], '-og')\n",
    "plt.plot(h[2].history['val_loss'], '-or')\n",
    "plt.plot(h[3].history['val_loss'], '-oc')\n",
    "\n",
    "\n",
    "# Showing legend\n",
    "plt.legend(['rgb_255_mean', 'rgb_255_mean_std',\n",
    "            'gray_255_mean', 'gray_255_mean_std'],\n",
    "           loc='center right',\n",
    "           fontsize='large')\n",
    "\n",
    "\n",
    "# Giving name to axes\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "\n",
    "\n",
    "# Giving name to the plot\n",
    "plt.title('Losses of 1st model for Traffic Signs Datasets', fontsize=16)\n",
    "\n",
    "\n",
    "# Saving plot\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "plt.savefig('ts' + '/' + 'losses_model_1_ts_dataset.png', dpi=500)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing list of scheduled learning rate for every epoch\n",
    "print(h[0].history['lr'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Plotting scheduled learning rate\n",
    "plt.plot(h[0].history['lr'], '-mo')\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traffic Signs dataset, 2nd model\n",
    "\n",
    "## Step 1: Loading saved 2nd model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining lists to collect models in\n",
    "model_rgb = []\n",
    "model_gray = []\n",
    "\n",
    "\n",
    "# Loading 2nd model for Traffic Signs dataset\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "for i in range(2):\n",
    "    model_rgb.append(load_model(full_path_to_Section5 + '/' + \n",
    "                                'ts' + '/' + \n",
    "                                'model_2_ts_rgb.h5'))\n",
    "    \n",
    "    model_gray.append(load_model(full_path_to_Section5 + '/' + \n",
    "                                 'ts' + '/' + \n",
    "                                 'model_2_ts_gray.h5'))\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Models are successfully loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing models' input shapes\n",
    "print(model_rgb[0].layers[0].input_shape)\n",
    "print()\n",
    "print(model_gray[0].layers[0].input_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traffic Signs dataset, 2nd model\n",
    "\n",
    "## Step 2: Setting up learning rate & epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining number of epochs\n",
    "epochs = 50\n",
    "\n",
    "\n",
    "# Defining schedule to update learning rate\n",
    "learning_rate = LearningRateScheduler(lambda x: 1e-2 * 0.95 ** (x + epochs), verbose=1)\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Number of epochs and schedule for learning rate are set successfully')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traffic Signs dataset, 2nd model\n",
    "\n",
    "## Step 3: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If you're using Nvidia GPU and 'cnngpu' environment, there might be an issue like:\n",
    "'''Failed to get convolution algorithm. This is probably because cuDNN failed to initialize'''\n",
    "# In this case, close all Jupyter Notebooks, close Terminal Window or Anaconda Prompt\n",
    "# Open again just this one Jupyter Notebook and run it\n",
    "\n",
    "\n",
    "# Preparing list with datasets' names\n",
    "datasets = ['dataset_ts_rgb_255_mean.hdf5',\n",
    "            'dataset_ts_rgb_255_mean_std.hdf5',\n",
    "            'dataset_ts_gray_255_mean.hdf5',\n",
    "            'dataset_ts_gray_255_mean_std.hdf5']\n",
    "\n",
    "\n",
    "# Defining list to collect results in\n",
    "h = []\n",
    "\n",
    "\n",
    "# Training 2nd model with all Traffic Signs datasets in a loop\n",
    "for i in range(4):\n",
    "    # Opening saved Traffic Signs dataset from HDF5 binary file\n",
    "    # Initiating File object\n",
    "    # Opening file in reading mode by 'r'\n",
    "    # (!) On Windows, it might need to change\n",
    "    # this: + '/' +\n",
    "    # to this: + '\\' +\n",
    "    # or to this: + '\\\\' +\n",
    "    with h5py.File(full_path_to_Section4 + '/' + 'ts' + '/' + datasets[i], 'r') as f:\n",
    "        # Extracting saved arrays for training by appropriate keys\n",
    "        # Saving them into new variables\n",
    "        x_train = f['x_train']  # HDF5 dataset\n",
    "        y_train = f['y_train']  # HDF5 dataset\n",
    "        # Converting them into Numpy arrays\n",
    "        x_train = np.array(x_train)  # Numpy arrays\n",
    "        y_train = np.array(y_train)  # Numpy arrays\n",
    "\n",
    "        # Extracting saved arrays for validation by appropriate keys\n",
    "        # Saving them into new variables\n",
    "        x_validation = f['x_validation']  # HDF5 dataset\n",
    "        y_validation = f['y_validation']  # HDF5 dataset\n",
    "        # Converting them into Numpy arrays\n",
    "        x_validation = np.array(x_validation)  # Numpy arrays\n",
    "        y_validation = np.array(y_validation)  # Numpy arrays\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Following dataset is successfully opened:        ', datasets[i])\n",
    "    \n",
    "    \n",
    "    # Preparing classes to be passed into the model\n",
    "    # Transforming them from vectors to binary matrices\n",
    "    # It is needed to set relationship between classes to be understood by the algorithm\n",
    "    # Such format is commonly used in training and predicting\n",
    "    y_train = to_categorical(y_train, num_classes = 43)\n",
    "    y_validation = to_categorical(y_validation, num_classes = 43)\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Binary matrices are successfully created:        ', datasets[i])\n",
    "\n",
    "\n",
    "    # Preparing filepath to save best weights\n",
    "    # (!) On Windows, it might need to change\n",
    "    # this: + '/' +\n",
    "    # to this: + '\\' +\n",
    "    # or to this: + '\\\\' +\n",
    "    best_weights_filepath = 'ts' + '/' + 'w_2' + datasets[i][7:-5] + '.h5'\n",
    "    \n",
    "    # Formatting options to save all weights for every epoch\n",
    "    # 'ts' + '/' + 'w_2' + datasets[i][7:-5] + '_{epoch:02d}_{val_accuracy:.4f}' + '.h5'\n",
    "    \n",
    "    # Defining schedule to save best weights\n",
    "    best_weights = ModelCheckpoint(filepath=best_weights_filepath,\n",
    "                                   save_weights_only=True,                                   \n",
    "                                   monitor='val_accuracy',\n",
    "                                   mode='max',\n",
    "                                   save_best_only=True,\n",
    "                                   period=1,\n",
    "                                   verbose=1)\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Schedule to save best weights is created:        ', datasets[i])\n",
    "    \n",
    "    \n",
    "    # Defining schedule to save intermediate weights\n",
    "    class CustomCallback(Callback):\n",
    "        # Constructor of the class\n",
    "        def __init__(self):\n",
    "            # Defining variable to be as a part of filename\n",
    "            self.filename = 0\n",
    "        \n",
    "        # Function that is called at the end of every batch\n",
    "        def on_train_batch_end(self, batch, logs=None):\n",
    "            # Checking if it is every 100th batch\n",
    "            if batch % 100 == 0:\n",
    "                # Preparing filepath to save intermediate weights\n",
    "                # (!) On Windows, it might need to change\n",
    "                # this: + '/' +\n",
    "                # to this: + '\\' +\n",
    "                # or to this: + '\\\\' +\n",
    "                intermediate_weights_filepath = 'ts' + '/' + \\\n",
    "                                                'intermediate' + '/' + \\\n",
    "                                                '{0:04d}'.format(self.filename) + \\\n",
    "                                                '_w_2' + \\\n",
    "                                                datasets[i][7:-5] + '.h5'\n",
    "                \n",
    "                # Getting weights only for the first convolutional layer\n",
    "                weights_layer_0 = self.model.get_weights()[0]\n",
    "                \n",
    "                # Saving obtained weights into new HDF5 binary file\n",
    "                # Initiating File object\n",
    "                # Creating file with current name\n",
    "                # Opening it in writing mode by 'w'\n",
    "                # (!) On Windows, it might need to change\n",
    "                # this: + '/' +\n",
    "                # to this: + '\\' +\n",
    "                # or to this: + '\\\\' +\n",
    "                with h5py.File(intermediate_weights_filepath, 'w') as f:\n",
    "                    # Calling method to create dataset of given shape and type\n",
    "                    # Saving Numpy array with weights from the first layer\n",
    "                    f.create_dataset('weights_layer_0', data=weights_layer_0, dtype='f')\n",
    "                    \n",
    "                # Increasing variable to be as a part of the next filename\n",
    "                self.filename += 1\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Schedule to save intermediate weights is created:', datasets[i])\n",
    "\n",
    "    \n",
    "    # Checking if RGB dataset is opened\n",
    "    if i <= 1:\n",
    "        # Checking if second RGB dataset is opened\n",
    "        if i == 1:\n",
    "            # Training RGB model with current dataset\n",
    "            temp = model_rgb[i].fit(x_train, y_train,\n",
    "                                    batch_size=50,\n",
    "                                    epochs=epochs,\n",
    "                                    validation_data=(x_validation, y_validation),\n",
    "                                    callbacks=[learning_rate, best_weights, CustomCallback()],\n",
    "                                    verbose=1)\n",
    "        \n",
    "        # Checking if first RGB dataset is opened\n",
    "        else:\n",
    "            # Training RGB model with current dataset\n",
    "            temp = model_rgb[i].fit(x_train, y_train,\n",
    "                                    batch_size=50,\n",
    "                                    epochs=epochs,\n",
    "                                    validation_data=(x_validation, y_validation),\n",
    "                                    callbacks=[learning_rate, best_weights],\n",
    "                                    verbose=1)\n",
    "\n",
    "        \n",
    "        # Adding results of 2nd model for current RGB dataset in the list\n",
    "        h.append(temp)\n",
    "        \n",
    "        \n",
    "        # Check points\n",
    "        print('2nd model for RGB is successfully trained on:    ', datasets[i])\n",
    "        print('Trained weights for RGB are saved successfully:  ', 'w_2' + datasets[i][7:-5] + '.h5')\n",
    "        print()\n",
    "    \n",
    "    # Checking if GRAY dataset is opened\n",
    "    elif i >= 2:\n",
    "        # Training GRAY model with current dataset\n",
    "        temp = model_gray[i-2].fit(x_train, y_train,\n",
    "                                   batch_size=50,\n",
    "                                   epochs=epochs,\n",
    "                                   validation_data=(x_validation, y_validation),\n",
    "                                   callbacks=[learning_rate, best_weights],\n",
    "                                   verbose=1)\n",
    "\n",
    "        \n",
    "        # Adding results of 2nd model for current GRAY dataset in the list\n",
    "        h.append(temp)\n",
    "        \n",
    "        \n",
    "        # Check points\n",
    "        print('2nd model for GRAY is successfully trained on:   ', datasets[i])\n",
    "        print('Trained weights for GRAY are saved successfully: ', 'w_2' + datasets[i][7:-5] + '.h5')\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traffic Signs dataset, 2nd model\n",
    "\n",
    "## Step 4: Showing and plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resulted accuracies of all Traffic Signs datasets for 2nd model\n",
    "for i in range(4):\n",
    "    print('T: {0:.5f},  V: {1:.5f},  D: {2}'.format(max(h[i].history['accuracy']),\n",
    "                                                    max(h[i].history['val_accuracy']),\n",
    "                                                    datasets[i][8:-5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing other parameters that history holds\n",
    "print(h[0].params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "\n",
    "\n",
    "# Plotting accuracies of all Traffic Signs datasets for 2nd model\n",
    "plt.plot(h[0].history['val_accuracy'], '-o')\n",
    "plt.plot(h[1].history['val_accuracy'], '-o')\n",
    "plt.plot(h[2].history['val_accuracy'], '-o')\n",
    "plt.plot(h[3].history['val_accuracy'], '-o')\n",
    "\n",
    "\n",
    "# Showing legend\n",
    "plt.legend(['rgb_255_mean', 'rgb_255_mean_std',\n",
    "            'gray_255_mean', 'gray_255_mean_std'],\n",
    "           loc='lower right',\n",
    "           fontsize='xx-large')\n",
    "\n",
    "\n",
    "# Giving name to axes\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "\n",
    "\n",
    "# Setting limit along Y axis\n",
    "plt.ylim(0.88, 0.994)\n",
    "\n",
    "\n",
    "# Giving name to the plot\n",
    "plt.title('Validation accuracies of 2nd model for Traffic Signs Datasets', fontsize=16)\n",
    "\n",
    "\n",
    "# Saving plot\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "plt.savefig('ts' + '/' + 'validation_model_2_ts_dataset.png', dpi=500)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "\n",
    "\n",
    "# Plotting training and validation losses of all Traffic Signs datasets for 2nd model\n",
    "plt.plot(h[0].history['loss'], '-ob')\n",
    "plt.plot(h[1].history['loss'], '-og')\n",
    "plt.plot(h[2].history['loss'], '-or')\n",
    "plt.plot(h[3].history['loss'], '-oc')\n",
    "\n",
    "plt.plot(h[0].history['val_loss'], '-ob')\n",
    "plt.plot(h[1].history['val_loss'], '-og')\n",
    "plt.plot(h[2].history['val_loss'], '-or')\n",
    "plt.plot(h[3].history['val_loss'], '-oc')\n",
    "\n",
    "\n",
    "# Showing legend\n",
    "plt.legend(['rgb_255_mean', 'rgb_255_mean_std',\n",
    "            'gray_255_mean', 'gray_255_mean_std'],\n",
    "           loc='center right',\n",
    "           fontsize='large')\n",
    "\n",
    "\n",
    "# Giving name to axes\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "\n",
    "\n",
    "# Giving name to the plot\n",
    "plt.title('Losses of 2nd model for Traffic Signs Datasets', fontsize=16)\n",
    "\n",
    "\n",
    "# Saving plot\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "plt.savefig('ts' + '/' + 'losses_model_2_ts_dataset.png', dpi=500)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing list of scheduled learning rate for every epoch\n",
    "print(h[0].history['lr'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Plotting scheduled learning rate\n",
    "plt.plot(h[0].history['lr'], '-mo')\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some comments\n",
    "\n",
    "To get more details for usage of 'ModelCheckpoint' class:  \n",
    "**print(help(ModelCheckpoint))**  \n",
    "  \n",
    "More details and examples are here:  \n",
    " - https://keras.io/api/callbacks/  \n",
    " - https://keras.io/api/callbacks/model_checkpoint/  \n",
    " - https://keras.io/guides/writing_your_own_callbacks/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(help(ModelCheckpoint))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
